# **Информационно-аналитическая система "Анализ Новостей"**   
    + название проекта в git fin-proj-01 https://github.com/ferosta/fin-proj-01   
    + Работа выполняется в рамках учебного курса "Инженер данных"  
    + Итоговая аттестация: вариант проекта №01 https://sprint.1t.ru/user/my-course/186/1097/10447  
!["Вид проекта в githab"](./pic/prj_github_view.png "gitHub - Вид проекта ") 
    



1. # Текущее состояние разработки
    * организован сбор данных из RSS источников;
    * организовано хранение данных сырого слоя (файлы в папках источников);
    * организована инициализирующая и инкрементальная загрузка данных в базу данных;
    * организовано получение обобщающей таблицы с данными из всех источников;
    * организована обобщающая категоризация и разбиение на группы новостей
    * разработаны скрипты для формировния витрины со следующими данными:  
        * по каждой категории все источники за все дни;  
        * по каждой категории все источники за сутки;  
        * по каждой категории каждый источник за сутки;  
        * по каждой категории каждый источник за все дни.
    
    - продолжается реализация оставшихся заданий для финального слоя витрин.  


1. # Комментарии по заданию

o   **Инициализирующий – загрузка полного слепка данных источника**  
+ источник через rss возвращает набор данных фиксированной длинны. Реализовано сохранение получаемых данных в виде отдельных файлов и последующаяя загрузка всех сохраненных файлов на этапе инициализации.

o   **Инкрементальный – загрузка дельты данных за прошедшие сутки**  
+ В cron добавлено выполнение скрипта по сбору данных из источников (сделано немного чаще, чем за сутки). После каждого получепния порции данных, она сразу добавляется в базу данных (Postgress) 

***Организовать правильную структуру хранения данных***  
o   **Сырой слой данных**    
+ реализован в форме папок для каждого RSS-источника и хранящихся там файлов с получаемыми в каждом запросе порциями данных. Имена папок - названия источников, имена файлов - таймстампы времени получения данных.  

o  **Промежуточный слой**
+ база данных Postgress (докер контейнер). Таблицы для каждого источника. Общая объединенная таблица. Таблица с добавленными группами категорий  

o   **Слой витрин**
+ (== на текущий момент дорабатываются последние элементы ==)  
Реализуется в базе данных Postgres в виде широкой таблицы, получаемой в результате SQL запросов к таблицам промежуточного слоя.  
***На текущий момент сделано***: 
>+ Название категории  
>+ Общее количество новостей из всех источников по данной категории за все время  
>+ Количество новостей данной категории для каждого из источников за все время  
>+ Общее количество новостей из всех источников по данной категории за последние сутки
>+ Количество новостей данной категории для каждого из источников за последние сутки  
>+ Среднее количество публикаций по данной категории в сутки   


1. # Описание элементов предлагаемого решения

* Сбор данных из RSS источников
    * список источников храниться в конфигруационном файле 
* Размещение первичных данных в виде файлов по папкам с названиями источников
    * названия папок источников получается из адреса источника - удаляется http, / заменяются на |
    * имена файлов - таймстам момента получения данных
    * формат файлов - json
    * работа с данными ведется с помощью датафреймов Pandas
* Инициализация основного хранилища - база данных Postgres
    * база поднята в докер-контейнере
    * запись в базу ведется средствами Pandas
    * В хранилище записываются все данные ранее, собранные и записанные в подпапки источников
* Инкрементальная загрузка данных
    * Получаем порцию информции из каждого RSS источника
    * записываем ее в виде файла
    * добавляем без дублирования строк в соответствующую SQL таблицу
* Оркестрация работы системы
    * в качестве оркестратора используется cron;
    * данные из источников собираются каждые 4 часа 
    * формирование витрины производится раз в сутки в 8:30
* Формирование витрины
    * таблицы данных всех источников сливаются в единую таблицу
    * единая таблица "обогащается" групповыми категориями
    * (групповые категории временно формируются в ручную на основании имеющихся данных)
    * подготавливается выборка нужных данных (длинная вертикальная таблица)
    * формируется сводная таблица(crosstab), которая и является требуемой витриной (широкая горизонтальная таблица)
## Предварительная витрина (проект)
![Предварительная витрина](./pic/pred_vitrine_01-02.png "Проект витрины 01-02")    
## Структура проекта витрины
![Предварительная витрина](./pic/struct_pred_vitrine_01-02.png "Структура Проекта витрины 01-02")  
    

1. # Файловая структура проекта
![Файловая структура](./pic/prj_structure.png "Структура проекта")  

- **config** - папка с файлом, где записаны ссылки на источники RSS;
- **data** - папка сырого слоя данных. Там хранятся файлы feedов RSS;
- **postgres** - папка с конфигурацией docker-compose для Postgres;
- **cron** - папка с копией строки инициализации cron регулярного сбора и загрузки данных;
- **prod** - папка с рабочими версиями скриптов;
- **category** - папка с таблицей обобщенных групп категорий;
- **LOG**..., **EBUG**... - логи выполнения скриптов;
- **my_rss_data.ipynb**, **my_rss_data.py** - разрабатываемые версии юпитер ноутбука и спаренного с ним посредством jupytertext скрипта.
- **my_rss_data_env.py** - конфигурационный файл для переключения между режимом разработки/отладки и режимом исопльзования.  
- **vitrines** - папка с копиями формируемых витрин в формате csv.

    
## Файловая структура сырого слоя
![Файловая структура сырых данных](./pic/prj_files_structure.png "Структура файлов сырых данных")  
## Файловая структура сырых данных для каждого источника RSS
![Файловая структура данных источника](./pic/prj_source_files.png "Сырые даннаые источика")  


1. # Структура базы данных проекта
## Таблицы базы данных 
![Таблицы базы данных](./pic/prj_db_structure.png "Список таблиц БД")
## Структура главной таблицы
![Структура главной таблицы](./pic/table-main_cat-shema.png "Главная таблица") 



1. # Задание

<!-- #raw -->
=====================================
> Проект № 1.  
> 
> Анализ публикуемых новостей  
> 
> Общая задача: создать ETL-процесс формирования витрин данных для анализа публикаций новостей.  
> 
> Подробное описание задачи:  
> 
> * Разработать скрипты загрузки данных в 2-х режимах:  
    - o   Инициализирующий – загрузка полного слепка данных источника  
    - o   Инкрементальный – загрузка дельты данных за прошедшие сутки  
> 
> * Организовать правильную структуру хранения данных  
    - o   Сырой слой данных  
    - o   Промежуточный слой  
    - o   Слой витрин  
> 
> * В качестве результата работы программного продукта необходимо написать скрипт, который формирует витрину данных следующего содержания:  
    + Суррогатный ключ категории  
    + Название категории  
    + Общее количество новостей из всех источников по данной категории за все время  
    + Количество новостей данной категории для каждого из источников за все время  
    + Общее количество новостей из всех источников по данной категории за последние сутки  
    + Количество новостей данной категории для каждого из источников за последние сутки  
    + Среднее количество публикаций по данной категории в сутки  
    - День, в который было сделано максимальное количество публикаций по данной новости  
    - Количество публикаций новостей данной категории по дням недели  

> * Дополнение:  
> 
> Т.к. в разных источниках названия и разнообразие категорий могут отличаться, вам необходимо привести все к единому виду.  
> 
> Источники:  
> 
> https://lenta.ru/rss/  
> https://www.vedomosti.ru/rss/news  
> https://tass.ru/rss/v2.xml  
<!-- #endraw -->

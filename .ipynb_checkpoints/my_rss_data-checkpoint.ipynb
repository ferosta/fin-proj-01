{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f853482b-446a-49da-b279-499c800f4faf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a99891b-2767-477e-bcc4-bba3142fd564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:25.392179Z",
     "iopub.status.busy": "2023-01-12T17:49:25.391674Z",
     "iopub.status.idle": "2023-01-12T17:49:27.630248Z",
     "shell.execute_reply": "2023-01-12T17:49:27.629249Z",
     "shell.execute_reply.started": "2023-01-12T17:49:25.392086Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rss_parser import Parser\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, MetaData,Table, Column, Numeric, Integer, VARCHAR, text, DateTime \n",
    "from sqlalchemy.engine import result\n",
    "\n",
    "import errno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990422e-fc88-4d41-9072-12c6bc776773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-26T13:24:01.425526Z",
     "iopub.status.busy": "2022-12-26T13:24:01.364450Z",
     "iopub.status.idle": "2022-12-26T13:24:02.598235Z",
     "shell.execute_reply": "2022-12-26T13:24:02.546571Z",
     "shell.execute_reply.started": "2022-12-26T13:24:01.405898Z"
    },
    "tags": []
   },
   "source": [
    "# Конфигурационные настройки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeeeedb-0d22-4e32-94e8-1ca3c15217db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-27T13:48:44.405275Z",
     "iopub.status.busy": "2022-12-27T13:48:44.404972Z",
     "iopub.status.idle": "2022-12-27T13:48:44.409898Z",
     "shell.execute_reply": "2022-12-27T13:48:44.409051Z",
     "shell.execute_reply.started": "2022-12-27T13:48:44.405243Z"
    },
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "## Глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "b733d7ab-e03a-4c4d-b4a1-07d10018bdc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:50:10.339264Z",
     "iopub.status.busy": "2023-01-12T17:50:10.338557Z",
     "iopub.status.idle": "2023-01-12T17:50:10.351778Z",
     "shell.execute_reply": "2023-01-12T17:50:10.350060Z",
     "shell.execute_reply.started": "2023-01-12T17:50:10.339213Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#импортируем путь и уровень лога - они разные для отладки и продакшына\n",
    "from my_rss_data_env import RUN_DIR, LOG_LEVEL\n",
    "\n",
    "# название программы - для логов\n",
    "PROG_NAME = 'MY_RSS_DATA'\n",
    "# LOG_LEVEL = 'INFO' # 'DEBUG' # \n",
    "RUN_DIR = os.path.abspath(RUN_DIR)\n",
    "\n",
    "# конфигурационные настройки\n",
    "CONFIG_FILE_NAME = os.path.join(RUN_DIR, u'config/rss_links.csv') \n",
    "DATA_DIR_NAME = os.path.join(RUN_DIR, u'data')\n",
    "MAIN_TABLE_NAME = \"main\"\n",
    "CATEGORY_FILE = os.path.join(RUN_DIR, u'./category/category.csv')\n",
    "CATEGORY_TABLE = \"category_map\"\n",
    "\n",
    "\n",
    "# подключение к Postgres - который развернут в докере на сервере\n",
    "PGS_LGIN = 'postgres'\n",
    "PGS_PSWD = 'postgres'\n",
    "PGS_DB = 'postgres'\n",
    "PGS_ADDR =  '172.17.0.1' #'192.168.144.9'\n",
    "PGS_PORT = 5440\n",
    "\n",
    "SQL_ENGINE = create_engine(f'postgresql://{PGS_LGIN}:{PGS_PSWD}@localhost:{PGS_PORT}/{PGS_DB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1170d9f-adf8-4715-bdf2-921cd7916d36",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "1e2951a8-7413-4892-a3d1-7047f1a911cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:50:14.305976Z",
     "iopub.status.busy": "2023-01-12T17:50:14.305075Z",
     "iopub.status.idle": "2023-01-12T17:50:14.321704Z",
     "shell.execute_reply": "2023-01-12T17:50:14.320660Z",
     "shell.execute_reply.started": "2023-01-12T17:50:14.305913Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# логирование\n",
    "# !!! лучше бы использовать loguru !!!\n",
    "import logging\n",
    "import logging.config\n",
    "\n",
    "\n",
    "dictLogConfig = {\n",
    "    \"version\":1,\n",
    "    \"handlers\":{\n",
    "        \"StreamHandler\":{\n",
    "            \"class\":\"logging.StreamHandler\",\n",
    "            \"formatter\":\"myFormatter\"\n",
    "        },\n",
    "        \"GlobalfileHandler\":{\n",
    "            \"class\":\"logging.handlers.RotatingFileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"LOG_{PROG_NAME}.LOG\",\n",
    "            \"backupCount\": 10\n",
    "        },\n",
    "        \"fileHandlerDEBUG\":{\n",
    "            \"class\":\"logging.FileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"DEBUG_{PROG_NAME}.LOG\"\n",
    "        },\n",
    "         \"fileHandlerINFO\":{\n",
    "            \"class\":\"logging.FileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"LOG_{PROG_NAME}.LOG\"\n",
    "        },\n",
    "    },\n",
    "    \"loggers\":{\n",
    "        \"DEBUG\":{\n",
    "            \"handlers\":[\"fileHandlerDEBUG\", \"StreamHandler\"],\n",
    "            \"level\":\"DEBUG\",\n",
    "        },\n",
    "        \"INFO\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\"],\n",
    "            \"level\":\"INFO\",\n",
    "        },\n",
    "        \"WARNING\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"WARNING\",\n",
    "        },\n",
    "        \"ERROR\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"ERROR\",\n",
    "        },\n",
    "        \"CRITICAL\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"CRITICAL\",\n",
    "        }\n",
    "    },\n",
    "    \"formatters\":{\n",
    "        \"myFormatter\":{\n",
    "            \"format\":\"%(asctime)s:%(name)s:%(levelname)s=>%(message)s<=%(filename)s->%(funcName)s[%(lineno)d]\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "logging.config.dictConfig(dictLogConfig)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(f'{LOG_LEVEL}.{PROG_NAME}')\n",
    "# logger = logging.getLogger(\"DEBUG.\"+PROG_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d17a85-f14e-4277-8cc7-c1a7c4d18bbf",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Чтение конфига с адресами источников РСС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847c1b74-ee66-4bf1-abf9-dd9100a0e0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:27.765836Z",
     "iopub.status.busy": "2023-01-12T17:49:27.765109Z",
     "iopub.status.idle": "2023-01-12T17:49:27.861058Z",
     "shell.execute_reply": "2023-01-12T17:49:27.860248Z",
     "shell.execute_reply.started": "2023-01-12T17:49:27.765762Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lenta.ru/rss/',\n",
       " 'https://www.vedomosti.ru/rss/news',\n",
       " 'https://tass.ru/rss/v2.xml',\n",
       " 'https://ria.ru/export/rss2/archive/index.xml',\n",
       " 'https://www.kommersant.ru/RSS/news.xml',\n",
       " 'https://rossaprimavera.ru/rss',\n",
       " 'https://regnum.ru/rss',\n",
       " 'https://www.cnews.ru/inc/rss/news.xml',\n",
       " 'https://hibinform.ru/feed/',\n",
       " 'https://habr.com/ru/rss/all/all/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# читаем конфиг со ссылками на источники\n",
    "def read_config(config=CONFIG_FILE_NAME):\n",
    "    \"\"\"читаем конфиг со ссылками на источники\n",
    "        config - имя файла с конфигом (если не в локальной директории то с путём)\n",
    "    \"\"\"\n",
    "    df_config = pd.read_csv(config, header=None  )\n",
    "    rss_urls = list(df_config[0])\n",
    "    logger.debug(f'Ссылки на источники прочитаны из {config}')\n",
    "    return rss_urls\n",
    "\n",
    "\n",
    "# Тест\n",
    "rss_urls = read_config()\n",
    "rss_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef5ad2-62d0-4712-b1a4-a525cfd22ac8",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Подготовка первичного хранилища для данных из источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea4fba1-4c6b-4e8e-bd5d-0ba78639381e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:27.863642Z",
     "iopub.status.busy": "2023-01-12T17:49:27.863278Z",
     "iopub.status.idle": "2023-01-12T17:49:27.931346Z",
     "shell.execute_reply": "2023-01-12T17:49:27.930054Z",
     "shell.execute_reply.started": "2023-01-12T17:49:27.863619Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rssname_to_dirname(rss_url:str):\n",
    "    \"\"\" из адреса ссылки на источник делает имя папки для хранения фидов из этого источника\n",
    "        Результат: название папки с фидами источника\n",
    "    \"\"\"\n",
    "    # rss_url = 'https://regnum.ru/rss'# 'https://ria.ru/export/rss2/archive/index.xml' #'https://lenta.ru/rss/' # rss_urls[0]\n",
    "    rss_dirname = rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") \n",
    "    # abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    return rss_dirname\n",
    "\n",
    "    \n",
    "# подготовить: проверить и если надо создать каталог под данные из источника\n",
    "def rss_dir_prepare(rss_url):\n",
    "    \"\"\" Проверить есть ли каталог для данного источника,\n",
    "        Если нет, то создать каталог для сохранения сведений из источника .\n",
    "        rss_url - ссылка на источник из конфиг-файла\n",
    "    \"\"\"\n",
    "    # получаем имя папки с данными из ссылки на источник\n",
    "    rss_dir_name = rssname_to_dirname(rss_url)# rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\")\n",
    "    logger.debug(f'Проверяется папка rss_dir_name = {rss_dir_name}')\n",
    "    \n",
    "    # полный путь до папки с данными\n",
    "    rss_full_dir_name = os.path.join(DATA_DIR_NAME , rss_dir_name ) \n",
    "    rss_abs_dir_name =  rss_full_dir_name #os.path.abspath(rss_full_dir_name)\n",
    "    \n",
    "    # если такой папки еще нет - то создаем\n",
    "    if not os.path.exists(rss_abs_dir_name):\n",
    "        os.mkdir(rss_abs_dir_name)\n",
    "        logger.debug(f'Создна папка {rss_abs_dir_name}')\n",
    "    \n",
    "    return rss_abs_dir_name\n",
    "\n",
    "# Тест:    \n",
    "# rss_url = 'https://lenta.ru/rss/' # rss_urls[0]\n",
    "# rss_dirname = rss_dir_prepare(rss_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93c1c4-29ea-40b8-bada-1e2556f71bd5",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Получение данных из источника по ссылке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea64357-7f3d-4e7f-a549-2239004d83b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:27.933865Z",
     "iopub.status.busy": "2023-01-12T17:49:27.933403Z",
     "iopub.status.idle": "2023-01-12T17:49:28.014572Z",
     "shell.execute_reply": "2023-01-12T17:49:28.012921Z",
     "shell.execute_reply.started": "2023-01-12T17:49:27.933792Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение данных из источника по ссылке rss_url \n",
    "def get_rss(url : str):\n",
    "    \"\"\" получение данных из источника по ссылке rss_url \n",
    "        Результат: словарь feed\n",
    "    \"\"\"\n",
    "    # получаем данны из источника - всю порцию,которую он отдает. Настроек по выбору времени там нет!\n",
    "    xml = get(url)\n",
    "    parser = Parser(xml=xml.content  ) \n",
    "    feed = parser.parse()\n",
    "    logger.debug(f'Данные из {url} получены. Кол-во записей: { len( feed.dict()[\"feed\"]) }. Код Ок: {xml.ok}')\n",
    "    return feed.dict()['feed']\n",
    "\n",
    "# Тест:\n",
    "# rss_url = 'https://lenta.ru/rss/' # rss_urls[0]\n",
    "# rss_feed = get_rss(rss_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bce3d6-4b63-4186-9f06-fff131be8e08",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Сохранение полученных из истончика данных RSS в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdbcb0c-e979-44ba-adca-229d9a745567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.018380Z",
     "iopub.status.busy": "2023-01-12T17:49:28.017809Z",
     "iopub.status.idle": "2023-01-12T17:49:28.108504Z",
     "shell.execute_reply": "2023-01-12T17:49:28.106821Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.018310Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# преобразование даты из строки в datetime с timezone\n",
    "def convert_to_tz_datetime(dt : str): \n",
    "    \"\"\" преобразование даты из строки в datetime с timezone\n",
    "    \"\"\"\n",
    "    # формат даты #'Sat, 24 Dec 2022 09:10:22 +0300'   \n",
    "    fmt = \"%a, %d %b %Y %H:%M:%S %z\" \n",
    "    # код таймзоны\n",
    "    tz = datetime.strptime('+0300', '%z').tzinfo\n",
    "    \n",
    "    rez = datetime.now().astimezone(tz).strptime(dt, fmt)\n",
    "    logger.debug(rez.strftime(fmt) )\n",
    "    return rez\n",
    "\n",
    "\n",
    "# сохранение полученного и распаршенного rss в файл\n",
    "def save_rss_feed(feed_dict : dict, dir_to_save :str):\n",
    "    \"\"\" сохранение полученного и распаршенного rss в файл\n",
    "        вх: rss_feed - словарь с новостями\n",
    "            dir_to_save - путь до директории сохранения\n",
    "    \"\"\"\n",
    "    # формирование имени файла, в который записывается порция данных rss\n",
    "    # текущий таймстамп - для уникального имени файла\n",
    "    now_timestamp = int(datetime.now().timestamp())\n",
    "    \n",
    "    # #даты первой и последней новости в порции рсс\n",
    "    # pub_date_to = convert_to_tz_datetime( rss_feed[0]['publish_date'] )\n",
    "    # pub_date_from = convert_to_tz_datetime( rss_feed[-1]['publish_date'] )\n",
    "\n",
    "    # #имя файла для сохранения порции рсс\n",
    "    # fmt = \"%Y-%m-%d_%H-%M-%S\"\n",
    "    # file_name_dic = {'to':pub_date_to.strftime(fmt), 'from': pub_date_from.strftime(fmt) }\n",
    "    # file_name_str = json.dumps(file_name_dic).replace(\": \",'|')\n",
    "    # file_name_str\n",
    "    \n",
    "    # сохранение полученной порции rss в директорию источника\n",
    "\n",
    "    # полное имя файла для записи\n",
    "    abs_filename = os.path.join(dir_to_save, str(now_timestamp) + '.json')\n",
    "    with open(abs_filename, mode=\"w\") as fp:\n",
    "        json.dump(feed_dict , fp )\n",
    "        logger.debug(f'Rss_feed записан в файл {abs_filename}')\n",
    "    \n",
    "    return abs_filename\n",
    "\n",
    "# Тест:    \n",
    "# rss_filename = save_rss_feed(rss_feed, rss_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4e8f2-4aae-4fdf-874f-df4bafc85fa0",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# ** Загрузка данных из всех источников RSS и запись их в файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7108bf18-6176-48d9-a31e-52d04014d50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.110457Z",
     "iopub.status.busy": "2023-01-12T17:49:28.110165Z",
     "iopub.status.idle": "2023-01-12T17:49:28.170336Z",
     "shell.execute_reply": "2023-01-12T17:49:28.168703Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.110425Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_rss_data():\n",
    "    \"\"\" Получение данных из всех источников и запись их в файлы\n",
    "        Для CRONa\n",
    "    \"\"\"\n",
    "    logger.info('=== Начало загрузки данных ===')\n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    for url in rss_urls:\n",
    "\n",
    "        # подготавливаем папки для хранения скачиваемых из РСС данных\n",
    "        dirname = rss_dir_prepare(url)\n",
    "\n",
    "        # получаем порцию данных по ссылке\n",
    "        feed = get_rss(url)\n",
    "\n",
    "        # сохраняем данные в заранее подготовленной папке\n",
    "        rez_filename = save_rss_feed(feed, dirname)\n",
    "        \n",
    "    logger.info(f'=== Данные загрузили. Кол-во источников {len(rss_urls)} ===')\n",
    "        \n",
    "\n",
    "# # Тест:\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     get_all_rss_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973f857-9cce-4f95-8988-55368b0977ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Инициализирующая Загрузка данных из файлов в хранилище (SQL БД)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d10eca-d4d8-4e3c-82e5-ed63dd0170a6",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Прочитать файл feed и сделать из него таблицу пандас"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63fbbd8-9aba-4ef0-a30b-ce73cfd158d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.172745Z",
     "iopub.status.busy": "2023-01-12T17:49:28.172507Z",
     "iopub.status.idle": "2023-01-12T17:49:28.244571Z",
     "shell.execute_reply": "2023-01-12T17:49:28.242815Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.172716Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# прочитать из фид-файла и записать в пандас датафрейм\n",
    "def feedfile_to_pandas(rss_url:str, rss_file_name:str):\n",
    "    \"\"\" Читает json файл с сохраненнымto_list преобразует его в таблицу пандас\n",
    "        rss_url - название папки с файлами-фидами источника\n",
    "        rss_file_name - имя файла с фидом\n",
    "        Результат: таблица пандас\n",
    "    \"\"\"\n",
    "    \n",
    "    # формируем полное имя файла\n",
    "    rss_dirname = rssname_to_dirname(rss_url) \n",
    "    rss_full_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    feed_filename = os.path.join(rss_full_dirname, rss_file_name)\n",
    "    \n",
    "    \n",
    "    # открываем первый файл - это самый новый, т.к. сотритовка обратная\n",
    "    feed=''\n",
    "    with open(feed_filename, 'r') as fp:\n",
    "        feed = json.load(fp)\n",
    "        logger.debug(f'Прочитали содержимое файла {feed_filename}. Кол-во записей: {len(feed)}')\n",
    "\n",
    "    # закидываем фид в пандас : колонки только те, которые нужны\n",
    "    columns = ['title', 'link', 'publish_date', 'category', 'description' ] # 'description_links', 'description_images', 'enclosure', 'itunes'\n",
    "    df = pd.json_normalize(feed)[columns]\n",
    "    # добавляем признак источника\n",
    "    df['source'] = rss_dirname\n",
    "    df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "    df['hash'] = pd.util.hash_pandas_object(df[['title','category', 'source']]).astype('str')\n",
    "    \n",
    "    logger.debug(f'Из файла {feed_filename} получили таблицу, кол-во строк {len(df)}.')\n",
    "    return df\n",
    "\n",
    "# тест feedfile_to_pandas\n",
    "# rss_url = 'https://regnum.ru/rss'\n",
    "# feed_filename = '1672120674.json'\n",
    "# df1 = feedfile_to_pandas(rss_url, feed_filename)\n",
    "# df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe173428-f5a0-4842-8f12-364b13e22a65",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "## 1. -- Начальная инициализация через Pandas: Объединить в Pandas все файлы из папки источника рсс и записать результат в хранилище"
   ]
  },
  {
   "cell_type": "raw",
   "id": "effa825a-558f-45a7-9a88-df9ed39af862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.610419Z",
     "iopub.status.busy": "2022-12-29T18:01:25.610016Z",
     "iopub.status.idle": "2022-12-29T18:01:25.680944Z",
     "shell.execute_reply": "2022-12-29T18:01:25.679807Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.610387Z"
    },
    "tags": []
   },
   "source": [
    "def join_all_feedfiles_pandas_sql(rss_url: str):\n",
    "    \"\"\" взять все файлы с фидами в папке рсс, объединить их, убрав повторения и приготовить к записи в хранилище (?БД)\n",
    "        Результат: таблица пандас с уникальными записями из всех файлов в папке источника\n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # получаем список сохраненных файлов\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Кол-во файлов: {len(list_dir)}. Список: {list_dir}')\n",
    "    \n",
    "    df_rez = pd.DataFrame()\n",
    "    \n",
    "    for rf in list_dir:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "        #дату из строки делаем датой\n",
    "        # df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "        \n",
    "        #для отладки инфо: превая и последняя запись датафрефма\n",
    "        str_fst = df.iloc[0,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        str_lst = df.iloc[-1,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        logger.debug(f'Таблица для файла:{rf}, строк:{len(df)}, нач.:{str_fst}, кон.:{str_lst}')\n",
    "        # объединяем полученное с имеющимся \n",
    "        if df_rez.empty:\n",
    "            df_rez = df\n",
    "            logger.debug(f'Начальная инициализация пустой таблицы')\n",
    "        df_rez = pd.concat([df_rez, df], ignore_index=True ).df_rez.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    logger.debug(f'Сформировали сводную таблицу для файлов в {abs_rss_dirname}. Кол-во строк: {len(df_rez)}')\n",
    "    # df_rez.drop_duplicates(ignore_index=True, inplace=True)\n",
    "    # logger.debug(f'После удаления дубликатов: кол-во строк: {len(df_rez)}')\n",
    "    \n",
    "    # добавляем результат в БД\n",
    "    df_rez.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace' )\n",
    "    logger.debug(f'Добавлено в БД в таблицу: {rss_dirname}')\n",
    "    \n",
    "    return df_rez\n",
    "\n",
    "# # тест\n",
    "# if \"DEBUG\" in logger.name:\n",
    "    # rss_url = 'https://regnum.ru/rss'#\n",
    "    # df_rez = join_all_feedfiles_pandas_sql(rss_url)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73c216-513f-417b-8426-b97f28a71438",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. ** Начальная инициализация через SQL: Каждый файлы из папки источника рсс добавить в SQL хранилище, убрав дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36de0afa-6564-4dcf-9fb7-2fd0ec183179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.247625Z",
     "iopub.status.busy": "2023-01-12T17:49:28.247244Z",
     "iopub.status.idle": "2023-01-12T17:49:28.434555Z",
     "shell.execute_reply": "2023-01-12T17:49:28.433054Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.247581Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_all_feedfiles_sql(rss_url: str):\n",
    "    \"\"\" брать по очереди файлы с фидами в папке рсс и вставлять в SQL таблицу, избегая повторений\n",
    "        Результат: SQL таблица с уникальными записями из всех файлов в папке источника\n",
    "        \n",
    "        от insert_newest_feedfiles_by_sql отличается только тем, что берутся все файлы , а там только превый.\n",
    "        Можно сделать одну функцию и через параметр управлять количеством файлов. \n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # поскольку это начальная инициализация, то имеющуюся SQL таблицу удаляем\n",
    "    q = f'DROP TABLE IF EXISTS \"{rss_dirname}\"'\n",
    "    with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "    \n",
    "    # получаем список сохраненных файлов\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Кол-во файлов: {len(list_dir)}. Список: {list_dir}')\n",
    "    \n",
    "    for rf in list_dir:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "                \n",
    "        # #для отладки инфо: превая и последняя запись датафрефма\n",
    "        # str_fst = df.iloc[0,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        # str_lst = df.iloc[-1,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        # logger.debug(f'Таблица для файла:{rf}, строк:{len(df)}, нач.:{str_fst}, кон.:{str_lst}')\n",
    "        # объединяем полученное с имеющимся \n",
    "        # if df_rez.empty:\n",
    "        #     df_rez = df\n",
    "        #     logger.debug(f'Начальная инициализация пустой таблицы')\n",
    "        # df_rez = pd.concat([df_rez, df], ignore_index=True ).df_rez.drop_duplicates(ignore_index=True)\n",
    "        \n",
    "        # если sql таблица с данными для этого источника еще не создана - создаем новую, иначе - дописываем\n",
    "        if rss_dirname not in sa.inspect(SQL_ENGINE).get_table_names():\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ОСНОВНУЮ таблицу для данного источника\n",
    "            df.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            # делаем первичным ключем - хэш, чтобы записи не повторялись\n",
    "            q = f'ALTER TABLE public.\"{rss_dirname}\" ADD CONSTRAINT \"{rss_dirname}_pk\" PRIMARY KEY (hash);'\n",
    "            \n",
    "        else:\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ВРЕМЕННУЮ таблицу\n",
    "            tmp_dbname = \"tmp.\"+rss_dirname\n",
    "            df.to_sql(tmp_dbname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            q = f'INSERT INTO \"{rss_dirname}\" SELECT * FROM \"{tmp_dbname}\"\\\n",
    "                                                WHERE hash NOT IN (SELECT hash FROM \"{rss_dirname}\");\\\n",
    "                                                DROP TABLE \"{tmp_dbname}\"'\n",
    "            \n",
    "        # выполняем сформированный SQl запрос\n",
    "        with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "            \n",
    "\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{rss_dirname}\"')\n",
    "    \n",
    "    # общее количество строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "            \n",
    "    logger.debug(f'Сформировали SQL таблицу \"{rss_dirname}\". Кол-во строк: {num_str}')\n",
    "    \n",
    "    return {rss_dirname:num_str}\n",
    "\n",
    "# # тест\n",
    "# rez_sql = ''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     rss_url = 'https://regnum.ru/rss'#\n",
    "#     rez_sql = insert_all_feedfiles_sql(rss_url)\n",
    "# rez_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474654bb-34a3-4878-8152-a32285c149f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. ** Инкрементальная загрузка свежей порции данных через SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da66b761-6640-4996-a48e-d088e6382a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.437899Z",
     "iopub.status.busy": "2023-01-12T17:49:28.437447Z",
     "iopub.status.idle": "2023-01-12T17:49:28.511073Z",
     "shell.execute_reply": "2023-01-12T17:49:28.509809Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.437842Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_newest_feedfiles_by_sql(rss_url: str):\n",
    "    \"\"\" взять самый свежий файл с фидом в папке рсс и встить в SQL таблицу, избегая повторений\n",
    "        Результат: SQL таблица с уникальными записями из всех файлов в папке источника\n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # надо бы проверить - существует такакая таблица или еще нет\n",
    "    \n",
    "    # получаем список сохраненных файлов - сортируем в порядке убывания времени - т.е. самый свежий файл будет первым\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    # если файлов не нашлось - страшно ругаемся\n",
    "    if len(list_dir) == 0:\n",
    "        logger.error(\"Стоп! Файлы для добавления в sql таблицу {rss_dirname} отсутствуют в папке {abs_rss_dirname}\")\n",
    "        raise IOError \n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Берем в работу самый свежий файл: {list_dir[0]}')\n",
    "    \n",
    "    for rf in list_dir[0:1]:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "        \n",
    "        # если sql таблица с данными для этого источника еще не создана - создаем новую, иначе - дописываем\n",
    "        if rss_dirname not in sa.inspect(SQL_ENGINE).get_table_names():\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ОСНОВНУЮ таблицу для данного источника\n",
    "            df.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            # делаем первичным ключем - хэш, чтобы записи не повторялись\n",
    "            q = f'ALTER TABLE public.\"{rss_dirname}\" ADD CONSTRAINT \"{rss_dirname}_pk\" PRIMARY KEY (hash);'\n",
    "            \n",
    "        else:\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ВРЕМЕННУЮ таблицу\n",
    "            tmp_dbname = \"tmp.\"+rss_dirname\n",
    "            df.to_sql(tmp_dbname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            q = f'INSERT INTO \"{rss_dirname}\" SELECT * FROM \"{tmp_dbname}\"\\\n",
    "                                                WHERE hash NOT IN (SELECT hash FROM \"{rss_dirname}\");\\\n",
    "                                                DROP TABLE \"{tmp_dbname}\"'\n",
    "            \n",
    "        # выполняем сформированный SQl запрос\n",
    "        with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "            \n",
    "\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{rss_dirname}\"')\n",
    "    \n",
    "    # финальное количство строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "    \n",
    "    logger.debug(f'Записали в SQL таблицу \"{rss_dirname}\". Кол-во строк: {num_str}')\n",
    "    \n",
    "    return {rss_dirname:num_str}\n",
    "\n",
    "# # тест\n",
    "# rez_sql = ''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     rss_url = 'https://regnum.ru/rss'#\n",
    "#     rez_sql = insert_newest_feedfiles_by_sql(rss_url)\n",
    "# rez_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172fcdbc-3d13-4d1a-950f-029db7457508",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "## -- Инициализирующая Загрузка данных из всех файлов всех папок источников RSS в SQL через PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67814561-a912-4b90-ac56-f20d1567e499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.513842Z",
     "iopub.status.busy": "2023-01-12T17:49:28.513175Z",
     "iopub.status.idle": "2023-01-12T17:49:28.660998Z",
     "shell.execute_reply": "2023-01-12T17:49:28.659386Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.513792Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_feeddirs_to_sql_by_pandas():\n",
    "    \"\"\" Загрузка всех данных из папок источников в SQL , через объединение их в pandas\"\"\"\n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    for url in rss_urls:\n",
    "\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        join_all_feedfiles_pandas_sql(url)\n",
    "\n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_all_feeddirs_to_sql_by_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe28ead-ab43-40cf-b5fc-a321795e493d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ** Инициализирующая Загрузка данных из ВСЕХ файлов всех папок источников RSS СРАЗУ в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122303b5-4fa6-4db5-a5a8-652410d2296b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.664662Z",
     "iopub.status.busy": "2023-01-12T17:49:28.663689Z",
     "iopub.status.idle": "2023-01-12T17:49:28.734275Z",
     "shell.execute_reply": "2023-01-12T17:49:28.733196Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.664578Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_feeddirs_directly_to_sql():\n",
    "    \"\"\" Инициализирующая Загрузка всех данных из папок источников непосресдвенно в SQL \n",
    "        Если таблица уже была, то она удаляется\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f'== Начало Инициализирующей загрузки')\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    str_num = dict()\n",
    "    \n",
    "    for url in rss_urls:\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        rez = insert_all_feedfiles_sql(url) \n",
    "        str_num.update(rez)\n",
    "        \n",
    "    logger.info(f'== Инициализирующая загрузка произведена. Кол-ва загруженных строк: {str_num}')\n",
    "        \n",
    "\n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_all_feeddirs_directly_to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11941ef5-f143-4af4-9ea0-2ec4b2375a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ** Инкрементальная Загрузка данных для всех источников RSS СРАЗУ в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e508fb7-a714-402c-b855-6ce0bbdde3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.738288Z",
     "iopub.status.busy": "2023-01-12T17:49:28.737900Z",
     "iopub.status.idle": "2023-01-12T17:49:28.744551Z",
     "shell.execute_reply": "2023-01-12T17:49:28.743583Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.738262Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_newest_feeddirs_directly_to_sql():\n",
    "    \"\"\" Загрузка самых новых данных (самый ноывй файл) из папок источников непосредственно в SQL  \"\"\"\n",
    "    \n",
    "    logger.info('=== Запись свежих данных в SQL таблицы ===')\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    str_num = dict()\n",
    "    \n",
    "    for url in rss_urls:\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        rez = insert_newest_feedfiles_by_sql(url)\n",
    "        str_num.update(rez)\n",
    "        \n",
    "    logger.info(f'=== Конец записи. Кол-ва записей в таблицах: {str_num} ===')\n",
    "    \n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_newest_feeddirs_directly_to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ded28-5144-40c6-acd5-01533fd8cc94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ** CRON : регулярное получение данных и записывание их в SQL базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb99207-30e2-4f97-a2c8-5786f2a45331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.745818Z",
     "iopub.status.busy": "2023-01-12T17:49:28.745593Z",
     "iopub.status.idle": "2023-01-12T17:49:28.849002Z",
     "shell.execute_reply": "2023-01-12T17:49:28.847473Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.745798Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cron():\n",
    "    \"\"\" реуглярно собираем данные из источников и тут же записываем их в SQL\"\"\"\n",
    "    get_all_rss_data()\n",
    "    \n",
    "    load_newest_feeddirs_directly_to_sql()\n",
    "    \n",
    "# тест\n",
    "if \"DEBUG\" in logger.name:\n",
    "    cron()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a0a6e-2297-4628-8872-dad946709d4b",
   "metadata": {},
   "source": [
    "# Получение объединенной SQL таблицы для всех источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15259442-4ecc-4d38-8d09-c1b6b319a6ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.853007Z",
     "iopub.status.busy": "2023-01-12T17:49:28.852383Z",
     "iopub.status.idle": "2023-01-12T17:49:28.912632Z",
     "shell.execute_reply": "2023-01-12T17:49:28.911687Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.852957Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_union_main_table(main_table=MAIN_TABLE_NAME):\n",
    "    \"\"\" сливает все имеющиеся таблицы с данными из источников в одну - \n",
    "        имя главной таблицы по умолчанию MAIN_TABLE_NAME\n",
    "    \"\"\"\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config()\n",
    "    \n",
    "    # если пришел пустой список - страшно ругаемся \n",
    "    if len(rss_urls) == 0:\n",
    "        logger.error(f'Стоп! Список источников пуст: {len(rss_urls)}')\n",
    "        raise IOError\n",
    "        \n",
    "    rss_tablenames = [rssname_to_dirname(url) for url in rss_urls]\n",
    "    \n",
    "    \n",
    "    # не удалось использовать SELECT * INTO ... в DBeaver работает, а здесь нет..\n",
    "    # приходится вручную создавать таблицу\n",
    "    # !!! если здесь добавить в конце COMMIT; то будет работать как в DBeaver!!!\n",
    "    qc = f'CREATE TABLE \"{main_table}\" (\\\n",
    "    title text NULL,\\\n",
    "    link text NULL,\\\n",
    "    publish_date timestamptz NULL,\\\n",
    "    category text NULL,\\\n",
    "    description text NULL,\\\n",
    "    \"source\" text NULL,\\\n",
    "    hash text NOT NULL,\\\n",
    "    CONSTRAINT \"main_table_pk\" PRIMARY KEY (hash)\\\n",
    "    );'\n",
    "    \n",
    "    q = f'DROP TABLE IF EXISTS \"{main_table}\";'\n",
    "          # DROP CONSTRAINT IF EXISTS \"main_table_pk\";'\n",
    "    \n",
    "    qq = f'INSERT INTO \"{main_table}\" SELECT * FROM \"{rss_tablenames[0]}\" '\n",
    "     \n",
    "    qqq = ' '.join( [f'UNION SELECT * FROM \"{u}\" ' for u in rss_tablenames[1:]] )\n",
    "    \n",
    "    qqqq = qq + qqq + ' ;'\n",
    "    \n",
    "    # with SQL_ENGINE.connect() as con:\n",
    "    #     res = con.execute(q)\n",
    "    #     res = con.execute(qqqq)\n",
    "    res = SQL_ENGINE.execute(q)\n",
    "    res = SQL_ENGINE.execute(qc)\n",
    "    res = SQL_ENGINE.execute(qqqq)\n",
    "    \n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{main_table}\"')\n",
    "    # общее количество строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "        \n",
    "        \n",
    "    logger.debug(f'Создали объединенную таблицу {main_table}. Количетво записей: {num_str}')\n",
    "    \n",
    "    ##должно получаться как-то вот так\n",
    "    #     SELECT * INTO main \n",
    "    # \t  \t\t   FROM \"habr.com|ru|rss|all|all|\"\n",
    "    # UNION SELECT * FROM \"hibinform.ru|feed|\"\n",
    "    # UNION SELECT * FROM \"lenta.ru|rss|\"\n",
    "    # UNION SELECT * FROM \"regnum.ru|rss\"\n",
    "    # UNION SELECT * FROM \"ria.ru|export|rss2|archive|index.xml\"\n",
    "    # UNION SELECT * FROM \"rossaprimavera.ru|rss\"\n",
    "    # UNION SELECT * FROM \"tass.ru|rss|v2.xml\"\n",
    "    # UNION SELECT * FROM \"www.cnews.ru|inc|rss|news.xml\"\n",
    "    # UNION SELECT * FROM \"www.kommersant.ru|RSS|news.xml\"\n",
    "    # UNION SELECT * FROM \"www.vedomosti.ru|rss|news\"\n",
    "        \n",
    "    # logger.debug(f'Строка запроса: {qqqq} ===')\n",
    "    # return qqqq\n",
    "        \n",
    "\n",
    "# # Тест:\n",
    "# q=''\n",
    "# qq=''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#      make_union_main_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e2d96-3913-4c4a-9fd6-c75ed9000324",
   "metadata": {},
   "source": [
    "# Группировка тематических рубрик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89b32c-dfb6-48fb-b4ce-cada5408392e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-30T20:33:41.007465Z",
     "iopub.status.busy": "2022-12-30T20:33:41.007054Z",
     "iopub.status.idle": "2022-12-30T20:33:41.012880Z",
     "shell.execute_reply": "2022-12-30T20:33:41.011641Z",
     "shell.execute_reply.started": "2022-12-30T20:33:41.007400Z"
    }
   },
   "source": [
    "## загрузка групп категорий из файла в таблицу SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "c29bedce-1fc0-4237-9cb8-bc2b387833ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:28.914375Z",
     "iopub.status.busy": "2023-01-12T17:49:28.914053Z",
     "iopub.status.idle": "2023-01-12T17:49:29.015760Z",
     "shell.execute_reply": "2023-01-12T17:49:29.014507Z",
     "shell.execute_reply.started": "2023-01-12T17:49:28.914341Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "''"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_category_map_from_file(cat_file =CATEGORY_FILE, cat_tab=CATEGORY_TABLE):\n",
    "    \"\"\" формирование таблицы сводных категорий из внешнего файла\n",
    "        .. пока такой вариант.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(cat_file, sep=';', header=None).rename(columns={0:'category', 1:'cat_group'})\n",
    "    \n",
    "    # если пришел пустой список - страшно ругаемся \n",
    "    if len(df) == 0:\n",
    "        logger.error(f'Стоп! Список категорий в файле {cat_file} пуст: {len(rss_urls)}')\n",
    "        raise IOError\n",
    "    \n",
    "    logger.debug(f'Прочитан файл сводных категорий. Кол-во записей: {len(df)}')\n",
    "    \n",
    "    df.to_sql(cat_tab, SQL_ENGINE, if_exists='replace', index=False)\n",
    "    \n",
    "#     # ключ\n",
    "#     q = f'ALTER TABLE public.\"{cat_tab}\" ADD CONSTRAINT \"{cat_tab}_pk\" PRIMARY KEY (category);'\n",
    "#     res = SQL_ENGINE.execute(q)\n",
    "\n",
    "    # добавление суррогатного ключа\n",
    "    q = f'ALTER TABLE {cat_tab} ADD COLUMN id SERIAL PRIMARY KEY;'\n",
    "    res = SQL_ENGINE.execute(q)\n",
    "    \n",
    "    # считаем общее количество получившихся строк\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{cat_tab}\"')\n",
    "    # общее количество строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "    \n",
    "    \n",
    "    logger.debug(f'Сводные категории загружены в SQL таблицу {cat_tab}. Кол-во записей: {num_str}')\n",
    "    \n",
    "    return  df\n",
    "\n",
    "#тест\n",
    "# df=''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     df = load_category_map_from_file()\n",
    "# df   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed387874-6aea-449e-a433-0f278d371967",
   "metadata": {},
   "source": [
    "## Добавление к главной обобщающей таблице групп категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "6743213c-b5e4-4184-8cc0-7966f323e913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:29.017865Z",
     "iopub.status.busy": "2023-01-12T17:49:29.017538Z",
     "iopub.status.idle": "2023-01-12T17:49:29.095435Z",
     "shell.execute_reply": "2023-01-12T17:49:29.093936Z",
     "shell.execute_reply.started": "2023-01-12T17:49:29.017830Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def add_cat_group_to_main_table():\n",
    "    \"\"\"\n",
    "        Добавление к главной обобщающей таблице групп категорий\n",
    "    \"\"\"\n",
    "     \n",
    "    \n",
    "    res = SQL_ENGINE.execute(f'\\\n",
    "            DROP TABLE IF EXISTS \"{MAIN_TABLE_NAME}_cat\";')\n",
    "            # DROP CONSTRAINT IF EXISTS \"main_table_cat_pk\";')\n",
    "    logger.debug(f'Удаление главной таблицы: {res}')\n",
    "\n",
    "    # формируем новую главную таблицу для вставки туда категрий\n",
    "    res = SQL_ENGINE.execute(f'\\\n",
    "        CREATE TABLE \"{MAIN_TABLE_NAME}_cat\" (\\\n",
    "            title text  NULL,\\\n",
    "            link text  NULL,\\\n",
    "            publish_date timestamptz  NULL,\\\n",
    "            category text NULL,\\\n",
    "            description text  NULL,\\\n",
    "            \"source\" text  NULL,\\\n",
    "            hash text  NULL,\\\n",
    "            cat_group text NULL,\\\n",
    "            cat_id INTEGER NULL,\\\n",
    "            CONSTRAINT \"main_table_cat_pk\" PRIMARY KEY (hash)\\\n",
    "        );\\\n",
    "    ')\n",
    "    logger.debug(f'Создание новой главной таблицы. Кол-во записей: {res.rowcount}')\n",
    "\n",
    "    # вставляем в основную таблицу категрии новостей\n",
    "    res = SQL_ENGINE.execute(f'\\\n",
    "        INSERT INTO \"{MAIN_TABLE_NAME}_cat\" \\\n",
    "        SELECT m.*, cm.cat_group, cm.id as cat_id FROM \"{MAIN_TABLE_NAME}\" m \\\n",
    "        LEFT JOIN \"{CATEGORY_TABLE}\" cm  ON m.category = cm.category;\\\n",
    "    ')\n",
    "    logger.debug(f'Добавеление в главную таблицу категрий. Кол-во записей: {res.rowcount}')\n",
    "    \n",
    "    \n",
    "    # т.к. в текущей версии костыльных ручных групп новостей будут не все группы, то \n",
    "    # пустые группы новостей пронумеруем и поименуем вручную еще раз\n",
    "    res = SQL_ENGINE.execute(f\"\\\n",
    "        UPDATE {MAIN_TABLE_NAME}_cat \\\n",
    "        SET cat_group = 'новые новости' \\\n",
    "            ,cat_id = 0 \\\n",
    "        WHERE cat_group IS NULL OR cat_id IS NULL; \\\n",
    "    \")\n",
    "    logger.debug(f'Корректировка неучтенных категорий. Кол-во записей: {res.rowcount}')\n",
    "\n",
    "    # q = f'INSERT INTO \"{MAIN_TABLE_NAME}_cat\" \\\n",
    "    # (SELECT m.*, cm.cat_group FROM \"{MAIN_TABLE_NAME}\" m \\\n",
    "    # LEFT JOIN \"{CATEGORY_TABLE}\" cm  ON m.category = cm.category) \\\n",
    "    # UNION ALL \\\n",
    "    # (SELECT m.*, cm.category as \"cat_group\" FROM \"{MAIN_TABLE_NAME}\" m, \"{CATEGORY_TABLE}\" cm  \\\n",
    "    # WHERE  m.category != cm.category)\\\n",
    "    # ;'\n",
    "\n",
    "    # print(q)\n",
    "    logger.debug(f'Сформирована главная таблица с категориями. Всего записей: {res.rowcount}')\n",
    "    return res\n",
    "    \n",
    "# #тест\n",
    "# res = ''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     res = add_cat_group_to_main_table()\n",
    "# # print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016f50a-7e94-4e50-a85e-2d8e2e1f4095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-27T13:37:59.864028Z",
     "iopub.status.busy": "2022-12-27T13:37:59.863537Z",
     "iopub.status.idle": "2022-12-27T13:37:59.882195Z",
     "shell.execute_reply": "2022-12-27T13:37:59.881373Z",
     "shell.execute_reply.started": "2022-12-27T13:37:59.863994Z"
    }
   },
   "source": [
    "## --Тематическое моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "8de3d575-5c65-401d-85ac-47909ea46db2",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-12T17:49:29.100897Z",
     "iopub.status.busy": "2023-01-12T17:49:29.099897Z",
     "iopub.status.idle": "2023-01-12T17:49:29.160169Z",
     "shell.execute_reply": "2023-01-12T17:49:29.158272Z",
     "shell.execute_reply.started": "2023-01-12T17:49:29.100816Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#TODO: если будет время, то заменю костыли ручной группировки по категориям на тематическое моделирование"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f4376ac-4883-4161-94ed-c4b670fde823",
   "metadata": {},
   "source": [
    "# Витрина"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d69dd60-4ce1-484c-bd39-b88faafb79eb",
   "metadata": {},
   "source": [
    "## вар1: ПредВитрина №№01-04\n",
    "Суррогатный ключ категории\n",
    "Название категории\n",
    "Общее количество новостей из всех источников по данной категории за все время\n",
    "Количество новостей данной категории для каждого из источников за все время"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17bb5983-cfe7-4b13-8fb2-e30f256df9f6",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f33c44cc-d5ee-48e3-a1c5-45554147ee2c",
   "metadata": {},
   "source": [
    "#### вар2: ПредВитрина №№01-06\n",
    "сначала собираем все нужные данные в вертикальную таблицу, потом делаем сводную таблицу - горизонтальную широкую витрину"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "448953bc-4316-42de-bffb-e7b2e5db62e0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T18:03:51.615255Z",
     "iopub.status.busy": "2023-01-13T18:03:51.614551Z",
     "iopub.status.idle": "2023-01-13T18:03:52.612659Z",
     "shell.execute_reply": "2023-01-13T18:03:52.611819Z",
     "shell.execute_reply.started": "2023-01-13T18:03:51.615217Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 21:03:52,600:DEBUG.MY_RSS_DATA:DEBUG=>Сформирована витрина (vitrine_06) по п.1 и п.2. Всего записей: -1<=1693787556.py->make_vitrine_01_06[138]\n",
      "2023-01-13 21:03:52,609:DEBUG.MY_RSS_DATA:DEBUG=>Витрина сохранена в файл /home/fedorov/mypy/dataeng/fin-prj-01/vitrines/1673633032-vitrine.csv<=1693787556.py->make_vitrine_01_06[148]\n"
     ]
    }
   ],
   "source": [
    "def make_vitrine_01_06(vitrine_name='vitrine_06'):\n",
    "    \"\"\"\n",
    "        -- Промежуточный результат: Витрина с данными задачи №01 и №02 03 04 05\n",
    "        -- выполнение одним запросом без сохранения промежуточных таблиц\n",
    "    \"\"\"\n",
    "    # TODO: надо бы еще формирование списков источников сделать динамическим\n",
    "    #\n",
    "    q = f\"\"\"\n",
    "        -- Задание №3: Общее количество новостей из всех источников по данной категории за последние сутки\n",
    "        -- Промежуточный результат: исходные данные с результатами №01 и №02 и №03 для сводной таблицы - витрины \n",
    "        -- цифры перед названием \"все источники\" нужны для правильного порядка вывода при формировании сводной таблицы\n",
    "        DROP TABLE IF EXISTS data_00_06;\n",
    "        WITH\n",
    "         src_list AS (SELECT DISTINCT \"source\" AS \"Источник\" FROM {MAIN_TABLE_NAME}_cat ORDER BY \"Источник\") --список источников\n",
    "        ,cat_list AS (SELECT DISTINCT cat_group AS \"Категория\" FROM {MAIN_TABLE_NAME}_cat ORDER BY \"Категория\")-- список категорий\n",
    "        ,cat_nlist AS(SELECT ROW_NUMBER() OVER() AS \"N\", \"Категория\"   FROM cat_list) -- нумерованный список категорий\n",
    "        ,all_src AS  (SELECT cat_group AS \"Категория\", '0Все источники' AS \"Источник\", count(*) AS \"Всего новостей\" -- цифра нужня для порядка колонок\n",
    "                      FROM {MAIN_TABLE_NAME}_cat GROUP BY cat_group ORDER BY cat_group)\n",
    "        ,each_src AS (SELECT cat_group AS \"Категория\", \"source\" as \"Источник\", count(*) AS \"Всего новостей\"\n",
    "                      FROM {MAIN_TABLE_NAME}_cat GROUP BY \"Категория\", \"Источник\" ORDER BY \"Категория\", \"Источник\")\n",
    "        ,day_all_src AS (SELECT cat_list.\"Категория\", '1День - Все источники' AS \"Источник\" , \"Всего новостей\" --цифра нужна для порядка колонок\n",
    "\t\t\t\t FROM (SELECT cat_group AS \"Категория\", '1День - Все источники' AS \"Источник\" , count(*) AS \"Всего новостей\"\t\n",
    "\t\t\t\t\t   FROM {MAIN_TABLE_NAME}_cat WHERE  publish_date > NOW() - INTERVAL '24 hours' GROUP BY \"Категория\") AS day_all_src -- выборка новостей за день\n",
    "\t\t\t\t RIGHT JOIN (SELECT * FROM cat_list) AS cat_list -- полный список категорий\n",
    "\t\t\t\t ON day_all_src.\"Категория\" = cat_list.\"Категория\")\n",
    "        ,day_each_src AS (\n",
    "\t\t\t\tSELECT   CASE WHEN cat_list.\"Категория\" != '[NULL]' -- дополняем до полного списка первым в списке\n",
    "\t\t\t\t\t\t\t   THEN cat_list.\"Категория\"\n",
    "\t\t\t\t\t\t\t   ELSE (SELECT * FROM cat_list LIMIT 1) END AS \"Категория\"\n",
    "\t\t\t\t\t\t, 7||src_list.\"Источник\"  -- дополняем до полного списка первым в списке\n",
    "\t\t\t\t\t\t, \"Всего новостей\" \n",
    "\t\t\t\tFROM (SELECT cat_group AS \"Категория\", \"source\" as \"Источник\" , count(*) AS \"Всего новостей\"\t\n",
    "\t\t\t\t\t  FROM {MAIN_TABLE_NAME}_cat WHERE  publish_date > NOW() - INTERVAL '24 hours' GROUP BY \"Категория\", \"Источник\"\n",
    "\t\t\t\t  \t ) AS day_list_each_src -- выборка новостей по каждому источнику за день\n",
    "\t\t\t\tRIGHT JOIN -- дополняем до полного списка категорий\n",
    "\t\t\t\t\t (SELECT DISTINCT cat_group AS \"Категория\" FROM {MAIN_TABLE_NAME}_cat ORDER BY \"Категория\") AS cat_list \n",
    "\t\t\t\tON day_list_each_src.\"Категория\" = cat_list.\"Категория\"\n",
    "\t\t\t\tRIGHT JOIN -- дополняем до полного списка источников\n",
    "\t\t\t\t\t (SELECT DISTINCT \"source\" AS \"Источник\" FROM {MAIN_TABLE_NAME}_cat ORDER BY \"Источник\") AS src_list\n",
    "\t\t\t\tON day_list_each_src.\"Источник\" = src_list.\"Источник\"\n",
    "\t\t\t\tORDER BY cat_list.\"Категория\", src_list.\"Источник\" )\n",
    "        ,day_avg_cat AS ( \n",
    "\t\t\t\tSELECT cat_group AS \"Категория\", '5День - Все источники' AS \"Источник\", avg(\"количество\")::integer AS \"4День: Среднее кол-во\"\n",
    "\t\t\t\tFROM (SELECT cat_group -- расфасовка категорий новостей по дням\n",
    "\t\t\t\t\t\t\t,(extract(epoch from NOW())::integer \n",
    "\t\t\t\t\t\t\t - extract(epoch from publish_date)::integer)\n",
    "\t\t\t\t\t\t\t /60/60/24 AS day_num -- номер дня: разница текущего таймстампа и таймстампа публикации \n",
    "\t\t\t\t\t\t\t, count(*) AS \"количество\"\n",
    "\t\t\t\t\tFROM {MAIN_TABLE_NAME}_cat mc \n",
    "\t\t\t\t\tGROUP BY cat_group , day_num\n",
    "\t\t\t\t\t) AS cat_group_days\n",
    "\t\t\t\tGROUP BY cat_group ORDER BY cat_group)\n",
    "            SELECT * \n",
    "            INTO data_00_06\n",
    "            FROM all_src -- все источники за все время\n",
    "        UNION\n",
    "            SELECT * FROM each_src -- каждый источник за все время\n",
    "        UNION \n",
    "            SELECT * FROM day_all_src -- все источники за сутки\n",
    "        UNION \n",
    "            SELECT * FROM day_each_src -- каждый источник за сутки\n",
    "        UNION \n",
    "            SELECT * FROM day_avg_cat  -- среднее за сутки все источники \n",
    "        ORDER BY \"Категория\",\"Источник\";\n",
    "        -------------------------------------------------------------------------\n",
    "        -- из сохранненой таблицы с объединенными данными делаем сводную таблицу\n",
    "        DROP TABLE IF EXISTS {vitrine_name};\n",
    "        \n",
    "        WITH -- тип данных дата не позволяет вставить этот кусок витрины в предыдущем запросе\n",
    "        cat_group_days AS (SELECT cat_group -- расфасовка категорий новостей по дням\n",
    "                                ,(extract(epoch from NOW())::integer \n",
    "                                 - extract(epoch from publish_date)::integer)\n",
    "                                 /60/60/24 AS day_num -- номер дня: разница текущего таймстампа и таймстампа публикации \n",
    "                                , count(*) AS \"количество\"\n",
    "                            FROM {MAIN_TABLE_NAME}_cat mc \n",
    "                            GROUP BY cat_group , day_num\n",
    "                            ORDER BY cat_group, \"количество\" DESC) \n",
    "        ,day_max_pub AS(SELECT cat_group AS \"Категория\", \"Источник\", current_date - day_num AS \"max_pub_day\"\n",
    "                        FROM ( -- списко категорий и максимального количества публикаций \n",
    "                              SELECT cat_group AS \"Категория\", '5День - Все источники' AS \"Источник\",  max(\"количество\")::integer AS \"День:Макс кол-во\"\n",
    "                              FROM (SELECT * FROM cat_group_days) AS cat_group_days\n",
    "                              GROUP BY cat_group) AS max_vals\n",
    "                        LEFT JOIN ( -- для получения номера дня по известному максимальному количеству публикаций в день\n",
    "                                    SELECT * FROM cat_group_days) AS cat_group_days2\n",
    "                        ON max_vals.\"Категория\" = cat_group_days2.cat_group AND max_vals.\"День:Макс кол-во\" = cat_group_days2.\"количество\"\n",
    "                        ORDER BY cat_group)\n",
    "        SELECT * \n",
    "        INTO {vitrine_name}\n",
    "        FROM \n",
    "        crosstab($$SELECT * FROM data_00_06 $$ -- выборка ключ-категория-значение\n",
    "                ,$$SELECT DISTINCT \"Источник\" FROM data_00_06 ORDER BY \"Источник\" $$ --полный список категорий (столбцов)\n",
    "        ) AS ct ( -- как сделать этот список динамически формируемым - пока не знаю\n",
    "             \"Категория\" TEXT\n",
    "            ,\"Всё время: Все источники\" INT8\n",
    "            ,\"Сутки: Все источники\" INT8\n",
    "            ,\"Среднее за сутки: Все источники\" INT8\n",
    "            ,\"День: 3habr.com|ru|rss|all|all|\" INT8\n",
    "            ,\"День: hibinform.ru|feed|\" INT8\n",
    "            ,\"День: lenta.ru|rss|\" INT8\n",
    "            ,\"День: regnum.ru|rss\" INT8\n",
    "            ,\"День: ria.ru|export|rss2|archive|index.xml\" INT8\n",
    "            ,\"День: rossaprimavera.ru|rss\" INT8\n",
    "            ,\"День: tass.ru|rss|v2.xml\" INT8\n",
    "            ,\"День: www.cnews.ru|inc|rss|news.xml\" INT8\n",
    "            ,\"День: www.kommersant.ru|RSS|news.xml\" INT8\n",
    "            ,\"День: www.vedomosti.ru|rss|news\" INT8\n",
    "            ,\"habr.com|ru|rss|all|all|\" INT8\n",
    "            ,\"hibinform.ru|feed|\" INT8\n",
    "            ,\"lenta.ru|rss|\" INT8\n",
    "            ,\"regnum.ru|rss\" INT8\n",
    "            ,\"ria.ru|export|rss2|archive|index.xml\" INT8\n",
    "            ,\"rossaprimavera.ru|rss\" INT8\n",
    "            ,\"tass.ru|rss|v2.xml\" INT8\n",
    "            ,\"www.cnews.ru|inc|rss|news.xml\" INT8\n",
    "            ,\"www.kommersant.ru|RSS|news.xml\" INT8\n",
    "            ,\"www.vedomosti.ru|rss|news\" INT8\n",
    "                    )\n",
    "        LEFT JOIN (SELECT \"Категория\" AS \"кат\", \"max_pub_day\" AS \"День макс.кол-ва публ.\" FROM day_max_pub) AS day_max_pub\n",
    "        ON ct.\"Категория\" = day_max_pub.\"кат\";\n",
    "        \n",
    "        -- после джойна остается одна лишняя колонка с категориями - удаляем\n",
    "        ALTER TABLE vitrine_06 DROP COLUMN \"кат\";\n",
    "        \n",
    "        COMMIT;\n",
    "     \"\"\"\n",
    "    \n",
    "    \n",
    "    res = SQL_ENGINE.execute(q)\n",
    "    # conn = SQL_ENGINE.connect()\n",
    "    # res = conn.execute(q)\n",
    "    # res = conn.execute(\"\"\"SELECT * FROM vitrine_03\"\"\")\n",
    "    # conn.commit_prepared()\n",
    "    # conn.close()\n",
    "    \n",
    "    # logger.debug(f'суппер-пуупер запрос: {res.rowcount}')\n",
    "\n",
    "   \n",
    "    logger.debug(f'Сформирована витрина ({vitrine_name}) по п.1 и п.2. Всего записей: {res.rowcount}')\n",
    "    # logger.debug(f'записей: {res.all()}')\n",
    "    \n",
    "    # сохраняем витрину в файл \n",
    "    now_timestamp = int(datetime.now().timestamp())\n",
    "    vitrine_dir = os.path.join(RUN_DIR, \"vitrines\")\n",
    "    vitrine_filename = os.path.join(vitrine_dir, str(now_timestamp) + '-vitrine.csv')\n",
    "    \n",
    "    df = pd.read_sql(f\"SELECT * FROM {vitrine_name}\", SQL_ENGINE)\n",
    "    df.to_csv(vitrine_filename )\n",
    "    logger.debug(f'Витрина сохранена в файл {vitrine_filename}')\n",
    "    \n",
    "    \n",
    "    return res\n",
    "    \n",
    "#тест\n",
    "res = ''\n",
    "if \"DEBUG\" in logger.name:\n",
    "    res = make_vitrine_01_06()\n",
    "# print (res)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8e73792-e570-4033-9877-65dbdf081d65",
   "metadata": {},
   "source": [
    "# ** **CRON**: Формирование главной витрины"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "e276a742-58b2-4d03-8d97-50c84fcccfbb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-13T18:05:58.550260Z",
     "iopub.status.busy": "2023-01-13T18:05:58.549940Z",
     "iopub.status.idle": "2023-01-13T18:06:11.602185Z",
     "shell.execute_reply": "2023-01-13T18:06:11.601430Z",
     "shell.execute_reply.started": "2023-01-13T18:05:58.550227Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-13 21:05:58,566:DEBUG.MY_RSS_DATA:DEBUG=>Ссылки на источники прочитаны из /home/fedorov/mypy/dataeng/fin-prj-01/config/rss_links.csv<=1700406913.py->read_config[8]\n",
      "2023-01-13 21:06:05,080:DEBUG.MY_RSS_DATA:DEBUG=>Создали объединенную таблицу main. Количетво записей: 120160<=2884603048.py->make_union_main_table[52]\n",
      "2023-01-13 21:06:05,081:DEBUG.MY_RSS_DATA:INFO=>Объединенная таблица с данными сформирована.<=1380753545.py->cron_vitrine[7]\n",
      "2023-01-13 21:06:05,100:DEBUG.MY_RSS_DATA:DEBUG=>Прочитан файл сводных категорий. Кол-во записей: 216<=2000337152.py->load_category_map_from_file[12]\n",
      "2023-01-13 21:06:05,452:DEBUG.MY_RSS_DATA:DEBUG=>Сводные категории загружены в SQL таблицу category_map. Кол-во записей: 216<=2000337152.py->load_category_map_from_file[30]\n",
      "2023-01-13 21:06:05,454:DEBUG.MY_RSS_DATA:INFO=>Группы категорий загружены.<=1380753545.py->cron_vitrine[11]\n",
      "2023-01-13 21:06:05,482:DEBUG.MY_RSS_DATA:DEBUG=>Удаление главной таблицы: <sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7f574981a590><=3885179813.py->add_cat_group_to_main_table[10]\n",
      "2023-01-13 21:06:05,551:DEBUG.MY_RSS_DATA:DEBUG=>Создание новой главной таблицы. Кол-во записей: -1<=3885179813.py->add_cat_group_to_main_table[27]\n",
      "2023-01-13 21:06:08,304:DEBUG.MY_RSS_DATA:DEBUG=>Добавеление в главную таблицу категрий. Кол-во записей: 120160<=3885179813.py->add_cat_group_to_main_table[35]\n",
      "2023-01-13 21:06:09,396:DEBUG.MY_RSS_DATA:DEBUG=>Корректировка неучтенных категорий. Кол-во записей: 7863<=3885179813.py->add_cat_group_to_main_table[46]\n",
      "2023-01-13 21:06:09,399:DEBUG.MY_RSS_DATA:DEBUG=>Сформирована главная таблица с категориями. Всего записей: 7863<=3885179813.py->add_cat_group_to_main_table[57]\n",
      "2023-01-13 21:06:09,400:DEBUG.MY_RSS_DATA:INFO=>Данным присвоены группы категорий.<=1380753545.py->cron_vitrine[16]\n",
      "2023-01-13 21:06:11,589:DEBUG.MY_RSS_DATA:DEBUG=>Сформирована витрина (vitrine_06) по п.1 и п.2. Всего записей: -1<=1693787556.py->make_vitrine_01_06[138]\n",
      "2023-01-13 21:06:11,598:DEBUG.MY_RSS_DATA:DEBUG=>Витрина сохранена в файл /home/fedorov/mypy/dataeng/fin-prj-01/vitrines/1673633171-vitrine.csv<=1693787556.py->make_vitrine_01_06[148]\n",
      "2023-01-13 21:06:11,599:DEBUG.MY_RSS_DATA:INFO=>Витрина сформирована:\n",
      "    За все время по всем источникам,\n",
      "    За сутки по всем источникам,\n",
      "    За сутки по каждому источнику,\n",
      "    За все время по каждому источнику,\n",
      "    Среднее за сутки за все время по всем источникам,\n",
      "    День максимального кол-ва публикаций по категории.<=1380753545.py->cron_vitrine[22]\n"
     ]
    }
   ],
   "source": [
    "def cron_vitrine():\n",
    "    \"\"\" последовательность действий для формирования финальной витрины\n",
    "        \n",
    "    \"\"\"\n",
    "    # формирование объединеной таблицы с данными по всем источникам\n",
    "    make_union_main_table()\n",
    "    logger.info('Объединенная таблица с данными сформирована.')\n",
    "    \n",
    "    # (костыли) загрузка в таблицу сформированного вручную файла с группами категрий\n",
    "    load_category_map_from_file()\n",
    "    logger.info('Группы категорий загружены.')\n",
    "    \n",
    "    \n",
    "    # добавление групп категорий к главной таблице с объединенными данными по всем источникам\n",
    "    add_cat_group_to_main_table()\n",
    "    logger.info('Данным присвоены группы категорий.')\n",
    "    \n",
    "    # формирование витрины для первых двух заданий: \n",
    "    # 01 : КАЖДАЯ категория, ВСЕ источники, ВСЕ дни\n",
    "    # 02 : КАЖДАЯ категория, КАЖДЫЙ источник, ВСЕ дни \n",
    "    make_vitrine_01_06()\n",
    "    logger.info('Витрина сформирована:\\n\\\n",
    "    За все время по всем источникам,\\n\\\n",
    "    За сутки по всем источникам,\\n\\\n",
    "    За сутки по каждому источнику,\\n\\\n",
    "    За все время по каждому источнику,\\n\\\n",
    "    Среднее за сутки за все время по всем источникам,\\n\\\n",
    "    День максимального кол-ва публикаций по категории.')\n",
    "    \n",
    "    \n",
    "#тест\n",
    "res = ''\n",
    "if \"DEBUG\" in logger.name:\n",
    "    cron_vitrine()\n",
    "# print (res)"
   ]
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

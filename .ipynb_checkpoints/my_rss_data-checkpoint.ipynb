{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f853482b-446a-49da-b279-499c800f4faf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a99891b-2767-477e-bcc4-bba3142fd564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:04.365584Z",
     "iopub.status.busy": "2023-01-06T13:07:04.365040Z",
     "iopub.status.idle": "2023-01-06T13:07:09.563770Z",
     "shell.execute_reply": "2023-01-06T13:07:09.562838Z",
     "shell.execute_reply.started": "2023-01-06T13:07:04.365474Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rss_parser import Parser\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, MetaData,Table, Column, Numeric, Integer, VARCHAR, text, DateTime \n",
    "from sqlalchemy.engine import result\n",
    "\n",
    "import errno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990422e-fc88-4d41-9072-12c6bc776773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-26T13:24:01.425526Z",
     "iopub.status.busy": "2022-12-26T13:24:01.364450Z",
     "iopub.status.idle": "2022-12-26T13:24:02.598235Z",
     "shell.execute_reply": "2022-12-26T13:24:02.546571Z",
     "shell.execute_reply.started": "2022-12-26T13:24:01.405898Z"
    },
    "tags": []
   },
   "source": [
    "# Конфигурационные настройки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeeeedb-0d22-4e32-94e8-1ca3c15217db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-27T13:48:44.405275Z",
     "iopub.status.busy": "2022-12-27T13:48:44.404972Z",
     "iopub.status.idle": "2022-12-27T13:48:44.409898Z",
     "shell.execute_reply": "2022-12-27T13:48:44.409051Z",
     "shell.execute_reply.started": "2022-12-27T13:48:44.405243Z"
    },
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "## Глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b733d7ab-e03a-4c4d-b4a1-07d10018bdc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:24.221102Z",
     "iopub.status.busy": "2023-01-06T13:07:24.220720Z",
     "iopub.status.idle": "2023-01-06T13:07:24.230331Z",
     "shell.execute_reply": "2023-01-06T13:07:24.228842Z",
     "shell.execute_reply.started": "2023-01-06T13:07:24.221004Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#импортируем путь и уровень лога - они разные для отладки и продакшына\n",
    "from my_rss_data_env import RUN_DIR, LOG_LEVEL\n",
    "\n",
    "# название программы - для логов\n",
    "PROG_NAME = 'MY_RSS_DATA'\n",
    "# LOG_LEVEL = 'INFO' # 'DEBUG' # \n",
    "RUN_DIR = os.path.abspath(RUN_DIR)\n",
    "\n",
    "# конфигурационные настройки\n",
    "CONFIG_FILE_NAME = os.path.join(RUN_DIR, u'config/rss_links.csv') \n",
    "DATA_DIR_NAME = os.path.join(RUN_DIR, u'data')\n",
    "MAIN_TABLE_NAME = \"main\"\n",
    "CATEGORY_FILE = os.path.join(RUN_DIR, u'./category/category.csv')\n",
    "CATEGORY_TABLE = \"category_map\"\n",
    "\n",
    "\n",
    "# подключение к Postgres - который развернут в докере на сервере\n",
    "PGS_LGIN = 'postgres'\n",
    "PGS_PSWD = 'postgres'\n",
    "PGS_DB = 'postgres'\n",
    "PGS_ADDR =  '172.17.0.1' #'192.168.144.9'\n",
    "PGS_PORT = 5440\n",
    "\n",
    "SQL_ENGINE = create_engine(f'postgresql://{PGS_LGIN}:{PGS_PSWD}@localhost:{PGS_PORT}/{PGS_DB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1170d9f-adf8-4715-bdf2-921cd7916d36",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1e2951a8-7413-4892-a3d1-7047f1a911cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:08:59.871160Z",
     "iopub.status.busy": "2023-01-06T13:08:59.870618Z",
     "iopub.status.idle": "2023-01-06T13:08:59.879833Z",
     "shell.execute_reply": "2023-01-06T13:08:59.879025Z",
     "shell.execute_reply.started": "2023-01-06T13:08:59.871135Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "##########################################\n",
    "# логирование\n",
    "# !!! лучше бы использовать loguru !!!\n",
    "import logging\n",
    "import logging.config\n",
    "\n",
    "\n",
    "dictLogConfig = {\n",
    "    \"version\":1,\n",
    "    \"handlers\":{\n",
    "        \"StreamHandler\":{\n",
    "            \"class\":\"logging.StreamHandler\",\n",
    "            \"formatter\":\"myFormatter\"\n",
    "        },\n",
    "        \"GlobalfileHandler\":{\n",
    "            \"class\":\"logging.handlers.RotatingFileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"LOG_{PROG_NAME}.LOG\",\n",
    "            \"backupCount\": 10\n",
    "        },\n",
    "        \"fileHandlerDEBUG\":{\n",
    "            \"class\":\"logging.FileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"DEBUG_{PROG_NAME}.LOG\"\n",
    "        },\n",
    "         \"fileHandlerINFO\":{\n",
    "            \"class\":\"logging.FileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"LOG_{PROG_NAME}.LOG\"\n",
    "        },\n",
    "    },\n",
    "    \"loggers\":{\n",
    "        \"DEBUG\":{\n",
    "            \"handlers\":[\"fileHandlerDEBUG\", \"StreamHandler\"],\n",
    "            \"level\":\"DEBUG\",\n",
    "        },\n",
    "        \"INFO\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\"],\n",
    "            \"level\":\"INFO\",\n",
    "        },\n",
    "        \"WARNING\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"WARNING\",\n",
    "        },\n",
    "        \"ERROR\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"ERROR\",\n",
    "        },\n",
    "        \"CRITICAL\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"CRITICAL\",\n",
    "        }\n",
    "    },\n",
    "    \"formatters\":{\n",
    "        \"myFormatter\":{\n",
    "            \"format\":\"%(asctime)s:%(name)s:%(levelname)s=>%(message)s<=%(filename)s->%(funcName)s[%(lineno)d]\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "logging.config.dictConfig(dictLogConfig)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(f'{LOG_LEVEL}.{PROG_NAME}')\n",
    "# logger = logging.getLogger(\"DEBUG.\"+PROG_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d17a85-f14e-4277-8cc7-c1a7c4d18bbf",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Чтение конфига с адресами источников РСС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847c1b74-ee66-4bf1-abf9-dd9100a0e0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:09.931952Z",
     "iopub.status.busy": "2023-01-06T13:07:09.931504Z",
     "iopub.status.idle": "2023-01-06T13:07:10.225183Z",
     "shell.execute_reply": "2023-01-06T13:07:10.224258Z",
     "shell.execute_reply.started": "2023-01-06T13:07:09.931929Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://lenta.ru/rss/',\n",
       " 'https://www.vedomosti.ru/rss/news',\n",
       " 'https://tass.ru/rss/v2.xml',\n",
       " 'https://ria.ru/export/rss2/archive/index.xml',\n",
       " 'https://www.kommersant.ru/RSS/news.xml',\n",
       " 'https://rossaprimavera.ru/rss',\n",
       " 'https://regnum.ru/rss',\n",
       " 'https://www.cnews.ru/inc/rss/news.xml',\n",
       " 'https://hibinform.ru/feed/',\n",
       " 'https://habr.com/ru/rss/all/all/']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# читаем конфиг со ссылками на источники\n",
    "def read_config(config=CONFIG_FILE_NAME):\n",
    "    \"\"\"читаем конфиг со ссылками на источники\n",
    "        config - имя файла с конфигом (если не в локальной директории то с путём)\n",
    "    \"\"\"\n",
    "    df_config = pd.read_csv(config, header=None  )\n",
    "    rss_urls = list(df_config[0])\n",
    "    logger.debug(f'Ссылки на источники прочитаны из {config}')\n",
    "    return rss_urls\n",
    "\n",
    "\n",
    "# Тест\n",
    "rss_urls = read_config()\n",
    "rss_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef5ad2-62d0-4712-b1a4-a525cfd22ac8",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Подготовка первичного хранилища для данных из источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea4fba1-4c6b-4e8e-bd5d-0ba78639381e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.226983Z",
     "iopub.status.busy": "2023-01-06T13:07:10.226702Z",
     "iopub.status.idle": "2023-01-06T13:07:10.234546Z",
     "shell.execute_reply": "2023-01-06T13:07:10.233428Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.226950Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rssname_to_dirname(rss_url:str):\n",
    "    \"\"\" из адреса ссылки на источник делает имя папки для хранения фидов из этого источника\n",
    "        Результат: название папки с фидами источника\n",
    "    \"\"\"\n",
    "    # rss_url = 'https://regnum.ru/rss'# 'https://ria.ru/export/rss2/archive/index.xml' #'https://lenta.ru/rss/' # rss_urls[0]\n",
    "    rss_dirname = rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") \n",
    "    # abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    return rss_dirname\n",
    "\n",
    "    \n",
    "# подготовить: проверить и если надо создать каталог под данные из источника\n",
    "def rss_dir_prepare(rss_url):\n",
    "    \"\"\" Проверить есть ли каталог для данного источника,\n",
    "        Если нет, то создать каталог для сохранения сведений из источника .\n",
    "        rss_url - ссылка на источник из конфиг-файла\n",
    "    \"\"\"\n",
    "    # получаем имя папки с данными из ссылки на источник\n",
    "    rss_dir_name = rssname_to_dirname(rss_url)# rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\")\n",
    "    logger.debug(f'Проверяется папка rss_dir_name = {rss_dir_name}')\n",
    "    \n",
    "    # полный путь до папки с данными\n",
    "    rss_full_dir_name = os.path.join(DATA_DIR_NAME , rss_dir_name ) \n",
    "    rss_abs_dir_name =  rss_full_dir_name #os.path.abspath(rss_full_dir_name)\n",
    "    \n",
    "    # если такой папки еще нет - то создаем\n",
    "    if not os.path.exists(rss_abs_dir_name):\n",
    "        os.mkdir(rss_abs_dir_name)\n",
    "        logger.debug(f'Создна папка {rss_abs_dir_name}')\n",
    "    \n",
    "    return rss_abs_dir_name\n",
    "\n",
    "# Тест:    \n",
    "# rss_url = 'https://lenta.ru/rss/' # rss_urls[0]\n",
    "# rss_dirname = rss_dir_prepare(rss_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93c1c4-29ea-40b8-bada-1e2556f71bd5",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Получение данных из источника по ссылке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea64357-7f3d-4e7f-a549-2239004d83b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.236456Z",
     "iopub.status.busy": "2023-01-06T13:07:10.236127Z",
     "iopub.status.idle": "2023-01-06T13:07:10.313173Z",
     "shell.execute_reply": "2023-01-06T13:07:10.311229Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.236422Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение данных из источника по ссылке rss_url \n",
    "def get_rss(url : str):\n",
    "    \"\"\" получение данных из источника по ссылке rss_url \n",
    "        Результат: словарь feed\n",
    "    \"\"\"\n",
    "    # получаем данны из источника - всю порцию,которую он отдает. Настроек по выбору времени там нет!\n",
    "    xml = get(url)\n",
    "    parser = Parser(xml=xml.content  ) \n",
    "    feed = parser.parse()\n",
    "    logger.debug(f'Данные из {url} получены. Кол-во записей: { len( feed.dict()[\"feed\"]) }. Код Ок: {xml.ok}')\n",
    "    return feed.dict()['feed']\n",
    "\n",
    "# Тест:\n",
    "# rss_url = 'https://lenta.ru/rss/' # rss_urls[0]\n",
    "# rss_feed = get_rss(rss_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bce3d6-4b63-4186-9f06-fff131be8e08",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Сохранение полученных из истончика данных RSS в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdbcb0c-e979-44ba-adca-229d9a745567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.315280Z",
     "iopub.status.busy": "2023-01-06T13:07:10.315017Z",
     "iopub.status.idle": "2023-01-06T13:07:10.398233Z",
     "shell.execute_reply": "2023-01-06T13:07:10.396570Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.315251Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# преобразование даты из строки в datetime с timezone\n",
    "def convert_to_tz_datetime(dt : str): \n",
    "    \"\"\" преобразование даты из строки в datetime с timezone\n",
    "    \"\"\"\n",
    "    # формат даты #'Sat, 24 Dec 2022 09:10:22 +0300'   \n",
    "    fmt = \"%a, %d %b %Y %H:%M:%S %z\" \n",
    "    # код таймзоны\n",
    "    tz = datetime.strptime('+0300', '%z').tzinfo\n",
    "    \n",
    "    rez = datetime.now().astimezone(tz).strptime(dt, fmt)\n",
    "    logger.debug(rez.strftime(fmt) )\n",
    "    return rez\n",
    "\n",
    "\n",
    "# сохранение полученного и распаршенного rss в файл\n",
    "def save_rss_feed(feed_dict : dict, dir_to_save :str):\n",
    "    \"\"\" сохранение полученного и распаршенного rss в файл\n",
    "        вх: rss_feed - словарь с новостями\n",
    "            dir_to_save - путь до директории сохранения\n",
    "    \"\"\"\n",
    "    # формирование имени файла, в который записывается порция данных rss\n",
    "    # текущий таймстамп - для уникального имени файла\n",
    "    now_timestamp = int(datetime.now().timestamp())\n",
    "    \n",
    "    # #даты первой и последней новости в порции рсс\n",
    "    # pub_date_to = convert_to_tz_datetime( rss_feed[0]['publish_date'] )\n",
    "    # pub_date_from = convert_to_tz_datetime( rss_feed[-1]['publish_date'] )\n",
    "\n",
    "    # #имя файла для сохранения порции рсс\n",
    "    # fmt = \"%Y-%m-%d_%H-%M-%S\"\n",
    "    # file_name_dic = {'to':pub_date_to.strftime(fmt), 'from': pub_date_from.strftime(fmt) }\n",
    "    # file_name_str = json.dumps(file_name_dic).replace(\": \",'|')\n",
    "    # file_name_str\n",
    "    \n",
    "    # сохранение полученной порции rss в директорию источника\n",
    "\n",
    "    # полное имя файла для записи\n",
    "    abs_filename = os.path.join(dir_to_save, str(now_timestamp) + '.json')\n",
    "    with open(abs_filename, mode=\"w\") as fp:\n",
    "        json.dump(feed_dict , fp )\n",
    "        logger.debug(f'Rss_feed записан в файл {abs_filename}')\n",
    "    \n",
    "    return abs_filename\n",
    "\n",
    "# Тест:    \n",
    "# rss_filename = save_rss_feed(rss_feed, rss_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4e8f2-4aae-4fdf-874f-df4bafc85fa0",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# ** Загрузка данных из всех источников RSS и запись их в файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7108bf18-6176-48d9-a31e-52d04014d50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.400718Z",
     "iopub.status.busy": "2023-01-06T13:07:10.400413Z",
     "iopub.status.idle": "2023-01-06T13:07:10.461295Z",
     "shell.execute_reply": "2023-01-06T13:07:10.460191Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.400669Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_rss_data():\n",
    "    \"\"\" Получение данных из всех источников и запись их в файлы\n",
    "        Для CRONa\n",
    "    \"\"\"\n",
    "    logger.info('=== Начало загрузки данных ===')\n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    for url in rss_urls:\n",
    "\n",
    "        # подготавливаем папки для хранения скачиваемых из РСС данных\n",
    "        dirname = rss_dir_prepare(url)\n",
    "\n",
    "        # получаем порцию данных по ссылке\n",
    "        feed = get_rss(url)\n",
    "\n",
    "        # сохраняем данные в заранее подготовленной папке\n",
    "        rez_filename = save_rss_feed(feed, dirname)\n",
    "        \n",
    "    logger.info(f'=== Данные загрузили. Кол-во источников {len(rss_urls)} ===')\n",
    "        \n",
    "\n",
    "# # Тест:\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     get_all_rss_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973f857-9cce-4f95-8988-55368b0977ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Инициализирующая Загрузка данных из файлов в хранилище (SQL БД)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d10eca-d4d8-4e3c-82e5-ed63dd0170a6",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Прочитать файл feed и сделать из него таблицу пандас"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63fbbd8-9aba-4ef0-a30b-ce73cfd158d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.464563Z",
     "iopub.status.busy": "2023-01-06T13:07:10.464251Z",
     "iopub.status.idle": "2023-01-06T13:07:10.548832Z",
     "shell.execute_reply": "2023-01-06T13:07:10.547392Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.464526Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# прочитать из фид-файла и записать в пандас датафрейм\n",
    "def feedfile_to_pandas(rss_url:str, rss_file_name:str):\n",
    "    \"\"\" Читает json файл с сохраненнымto_list преобразует его в таблицу пандас\n",
    "        rss_url - название папки с файлами-фидами источника\n",
    "        rss_file_name - имя файла с фидом\n",
    "        Результат: таблица пандас\n",
    "    \"\"\"\n",
    "    \n",
    "    # формируем полное имя файла\n",
    "    rss_dirname = rssname_to_dirname(rss_url) \n",
    "    rss_full_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    feed_filename = os.path.join(rss_full_dirname, rss_file_name)\n",
    "    \n",
    "    \n",
    "    # открываем первый файл - это самый новый, т.к. сотритовка обратная\n",
    "    feed=''\n",
    "    with open(feed_filename, 'r') as fp:\n",
    "        feed = json.load(fp)\n",
    "        logger.debug(f'Прочитали содержимое файла {feed_filename}. Кол-во записей: {len(feed)}')\n",
    "\n",
    "    # закидываем фид в пандас : колонки только те, которые нужны\n",
    "    columns = ['title', 'link', 'publish_date', 'category', 'description' ] # 'description_links', 'description_images', 'enclosure', 'itunes'\n",
    "    df = pd.json_normalize(feed)[columns]\n",
    "    # добавляем признак источника\n",
    "    df['source'] = rss_dirname\n",
    "    df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "    df['hash'] = pd.util.hash_pandas_object(df[['title','category', 'source']]).astype('str')\n",
    "    \n",
    "    logger.debug(f'Из файла {feed_filename} получили таблицу, кол-во строк {len(df)}.')\n",
    "    return df\n",
    "\n",
    "# тест feedfile_to_pandas\n",
    "# rss_url = 'https://regnum.ru/rss'\n",
    "# feed_filename = '1672120674.json'\n",
    "# df1 = feedfile_to_pandas(rss_url, feed_filename)\n",
    "# df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe173428-f5a0-4842-8f12-364b13e22a65",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "## 1. -- Начальная инициализация через Pandas: Объединить в Pandas все файлы из папки источника рсс и записать результат в хранилище"
   ]
  },
  {
   "cell_type": "raw",
   "id": "effa825a-558f-45a7-9a88-df9ed39af862",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.610419Z",
     "iopub.status.busy": "2022-12-29T18:01:25.610016Z",
     "iopub.status.idle": "2022-12-29T18:01:25.680944Z",
     "shell.execute_reply": "2022-12-29T18:01:25.679807Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.610387Z"
    },
    "tags": []
   },
   "source": [
    "def join_all_feedfiles_pandas_sql(rss_url: str):\n",
    "    \"\"\" взять все файлы с фидами в папке рсс, объединить их, убрав повторения и приготовить к записи в хранилище (?БД)\n",
    "        Результат: таблица пандас с уникальными записями из всех файлов в папке источника\n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # получаем список сохраненных файлов\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Кол-во файлов: {len(list_dir)}. Список: {list_dir}')\n",
    "    \n",
    "    df_rez = pd.DataFrame()\n",
    "    \n",
    "    for rf in list_dir:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "        #дату из строки делаем датой\n",
    "        # df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "        \n",
    "        #для отладки инфо: превая и последняя запись датафрефма\n",
    "        str_fst = df.iloc[0,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        str_lst = df.iloc[-1,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        logger.debug(f'Таблица для файла:{rf}, строк:{len(df)}, нач.:{str_fst}, кон.:{str_lst}')\n",
    "        # объединяем полученное с имеющимся \n",
    "        if df_rez.empty:\n",
    "            df_rez = df\n",
    "            logger.debug(f'Начальная инициализация пустой таблицы')\n",
    "        df_rez = pd.concat([df_rez, df], ignore_index=True ).df_rez.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    logger.debug(f'Сформировали сводную таблицу для файлов в {abs_rss_dirname}. Кол-во строк: {len(df_rez)}')\n",
    "    # df_rez.drop_duplicates(ignore_index=True, inplace=True)\n",
    "    # logger.debug(f'После удаления дубликатов: кол-во строк: {len(df_rez)}')\n",
    "    \n",
    "    # добавляем результат в БД\n",
    "    df_rez.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace' )\n",
    "    logger.debug(f'Добавлено в БД в таблицу: {rss_dirname}')\n",
    "    \n",
    "    return df_rez\n",
    "\n",
    "# # тест\n",
    "# if \"DEBUG\" in logger.name:\n",
    "    # rss_url = 'https://regnum.ru/rss'#\n",
    "    # df_rez = join_all_feedfiles_pandas_sql(rss_url)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73c216-513f-417b-8426-b97f28a71438",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. ** Начальная инициализация через SQL: Каждый файлы из папки источника рсс добавить в SQL хранилище, убрав дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "36de0afa-6564-4dcf-9fb7-2fd0ec183179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.554641Z",
     "iopub.status.busy": "2023-01-06T13:07:10.554406Z",
     "iopub.status.idle": "2023-01-06T13:07:10.612477Z",
     "shell.execute_reply": "2023-01-06T13:07:10.611468Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.554609Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_all_feedfiles_sql(rss_url: str):\n",
    "    \"\"\" брать по очереди файлы с фидами в папке рсс и вставлять в SQL таблицу, избегая повторений\n",
    "        Результат: SQL таблица с уникальными записями из всех файлов в папке источника\n",
    "        \n",
    "        от insert_newest_feedfiles_by_sql отличается только тем, что берутся все файлы , а там только превый.\n",
    "        Можно сделать одну функцию и через параметр управлять количеством файлов. \n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # поскольку это начальная инициализация, то имеющуюся SQL таблицу удаляем\n",
    "    q = f'DROP TABLE IF EXISTS \"{rss_dirname}\"'\n",
    "    with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "    \n",
    "    # получаем список сохраненных файлов\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Кол-во файлов: {len(list_dir)}. Список: {list_dir}')\n",
    "    \n",
    "    for rf in list_dir:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "                \n",
    "        # #для отладки инфо: превая и последняя запись датафрефма\n",
    "        # str_fst = df.iloc[0,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        # str_lst = df.iloc[-1,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        # logger.debug(f'Таблица для файла:{rf}, строк:{len(df)}, нач.:{str_fst}, кон.:{str_lst}')\n",
    "        # объединяем полученное с имеющимся \n",
    "        # if df_rez.empty:\n",
    "        #     df_rez = df\n",
    "        #     logger.debug(f'Начальная инициализация пустой таблицы')\n",
    "        # df_rez = pd.concat([df_rez, df], ignore_index=True ).df_rez.drop_duplicates(ignore_index=True)\n",
    "        \n",
    "        # если sql таблица с данными для этого источника еще не создана - создаем новую, иначе - дописываем\n",
    "        if rss_dirname not in sa.inspect(SQL_ENGINE).get_table_names():\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ОСНОВНУЮ таблицу для данного источника\n",
    "            df.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            # делаем первичным ключем - хэш, чтобы записи не повторялись\n",
    "            q = f'ALTER TABLE public.\"{rss_dirname}\" ADD CONSTRAINT \"{rss_dirname}_pk\" PRIMARY KEY (hash);'\n",
    "            \n",
    "        else:\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ВРЕМЕННУЮ таблицу\n",
    "            tmp_dbname = \"tmp.\"+rss_dirname\n",
    "            df.to_sql(tmp_dbname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            q = f'INSERT INTO \"{rss_dirname}\" SELECT * FROM \"{tmp_dbname}\"\\\n",
    "                                                WHERE hash NOT IN (SELECT hash FROM \"{rss_dirname}\");\\\n",
    "                                                DROP TABLE \"{tmp_dbname}\"'\n",
    "            \n",
    "        # выполняем сформированный SQl запрос\n",
    "        with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "            \n",
    "\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{rss_dirname}\"')\n",
    "    \n",
    "    # общее количество строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "            \n",
    "    logger.debug(f'Сформировали SQL таблицу \"{rss_dirname}\". Кол-во строк: {num_str}')\n",
    "    \n",
    "    return {rss_dirname:num_str}\n",
    "\n",
    "# # тест\n",
    "# rez_sql = ''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     rss_url = 'https://regnum.ru/rss'#\n",
    "#     rez_sql = insert_all_feedfiles_sql(rss_url)\n",
    "# rez_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474654bb-34a3-4878-8152-a32285c149f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. ** Инкрементальная загрузка свежей порции данных через SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "da66b761-6640-4996-a48e-d088e6382a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.614705Z",
     "iopub.status.busy": "2023-01-06T13:07:10.614050Z",
     "iopub.status.idle": "2023-01-06T13:07:10.698082Z",
     "shell.execute_reply": "2023-01-06T13:07:10.697149Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.614680Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_newest_feedfiles_by_sql(rss_url: str):\n",
    "    \"\"\" взять самый свежий файл с фидом в папке рсс и встить в SQL таблицу, избегая повторений\n",
    "        Результат: SQL таблица с уникальными записями из всех файлов в папке источника\n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # надо бы проверить - существует такакая таблица или еще нет\n",
    "    \n",
    "    # получаем список сохраненных файлов - сортируем в порядке убывания времени - т.е. самый свежий файл будет первым\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    # если файлов не нашлось - страшно ругаемся\n",
    "    if len(list_dir) == 0:\n",
    "        logger.error(\"Стоп! Файлы для добавления в sql таблицу {rss_dirname} отсутствуют в папке {abs_rss_dirname}\")\n",
    "        raise IOError \n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Берем в работу самый свежий файл: {list_dir[0]}')\n",
    "    \n",
    "    for rf in list_dir[0:1]:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "        \n",
    "        # если sql таблица с данными для этого источника еще не создана - создаем новую, иначе - дописываем\n",
    "        if rss_dirname not in sa.inspect(SQL_ENGINE).get_table_names():\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ОСНОВНУЮ таблицу для данного источника\n",
    "            df.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            # делаем первичным ключем - хэш, чтобы записи не повторялись\n",
    "            q = f'ALTER TABLE public.\"{rss_dirname}\" ADD CONSTRAINT \"{rss_dirname}_pk\" PRIMARY KEY (hash);'\n",
    "            \n",
    "        else:\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ВРЕМЕННУЮ таблицу\n",
    "            tmp_dbname = \"tmp.\"+rss_dirname\n",
    "            df.to_sql(tmp_dbname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            q = f'INSERT INTO \"{rss_dirname}\" SELECT * FROM \"{tmp_dbname}\"\\\n",
    "                                                WHERE hash NOT IN (SELECT hash FROM \"{rss_dirname}\");\\\n",
    "                                                DROP TABLE \"{tmp_dbname}\"'\n",
    "            \n",
    "        # выполняем сформированный SQl запрос\n",
    "        with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "            \n",
    "\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{rss_dirname}\"')\n",
    "    \n",
    "    # финальное количство строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "    \n",
    "    logger.debug(f'Записали в SQL таблицу \"{rss_dirname}\". Кол-во строк: {num_str}')\n",
    "    \n",
    "    return {rss_dirname:num_str}\n",
    "\n",
    "# # тест\n",
    "# rez_sql = ''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     rss_url = 'https://regnum.ru/rss'#\n",
    "#     rez_sql = insert_newest_feedfiles_by_sql(rss_url)\n",
    "# rez_sql"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172fcdbc-3d13-4d1a-950f-029db7457508",
   "metadata": {
    "incorrectly_encoded_metadata": "tags=[] jp-MarkdownHeadingCollapsed=true"
   },
   "source": [
    "## -- Инициализирующая Загрузка данных из всех файлов всех папок источников RSS в SQL через PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "67814561-a912-4b90-ac56-f20d1567e499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.699909Z",
     "iopub.status.busy": "2023-01-06T13:07:10.699719Z",
     "iopub.status.idle": "2023-01-06T13:07:10.768356Z",
     "shell.execute_reply": "2023-01-06T13:07:10.766839Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.699878Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_feeddirs_to_sql_by_pandas():\n",
    "    \"\"\" Загрузка всех данных из папок источников в SQL , через объединение их в pandas\"\"\"\n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    for url in rss_urls:\n",
    "\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        join_all_feedfiles_pandas_sql(url)\n",
    "\n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_all_feeddirs_to_sql_by_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe28ead-ab43-40cf-b5fc-a321795e493d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ** Инициализирующая Загрузка данных из ВСЕХ файлов всех папок источников RSS СРАЗУ в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "122303b5-4fa6-4db5-a5a8-652410d2296b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.772062Z",
     "iopub.status.busy": "2023-01-06T13:07:10.771327Z",
     "iopub.status.idle": "2023-01-06T13:07:10.849982Z",
     "shell.execute_reply": "2023-01-06T13:07:10.848183Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.771809Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_feeddirs_directly_to_sql():\n",
    "    \"\"\" Инициализирующая Загрузка всех данных из папок источников непосресдвенно в SQL \n",
    "        Если таблица уже была, то она удаляется\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f'== Начало Инициализирующей загрузки')\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    str_num = dict()\n",
    "    \n",
    "    for url in rss_urls:\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        rez = insert_all_feedfiles_sql(url) \n",
    "        str_num.update(rez)\n",
    "        \n",
    "    logger.info(f'== Инициализирующая загрузка произведена. Кол-ва загруженных строк: {str_num}')\n",
    "        \n",
    "\n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_all_feeddirs_directly_to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11941ef5-f143-4af4-9ea0-2ec4b2375a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ** Инкрементальная Загрузка данных для всех источников RSS СРАЗУ в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "1e508fb7-a714-402c-b855-6ce0bbdde3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.851748Z",
     "iopub.status.busy": "2023-01-06T13:07:10.851468Z",
     "iopub.status.idle": "2023-01-06T13:07:10.861999Z",
     "shell.execute_reply": "2023-01-06T13:07:10.860456Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.851715Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_newest_feeddirs_directly_to_sql():\n",
    "    \"\"\" Загрузка самых новых данных (самый ноывй файл) из папок источников непосредственно в SQL  \"\"\"\n",
    "    \n",
    "    logger.info('=== Запись свежих данных в SQL таблицы ===')\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    str_num = dict()\n",
    "    \n",
    "    for url in rss_urls:\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        rez = insert_newest_feedfiles_by_sql(url)\n",
    "        str_num.update(rez)\n",
    "        \n",
    "    logger.info(f'=== Конец записи. Кол-ва записей в таблицах: {str_num} ===')\n",
    "    \n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_newest_feeddirs_directly_to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ded28-5144-40c6-acd5-01533fd8cc94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ** CRON : регулярное получение данных и записывание их в SQL базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "eeb99207-30e2-4f97-a2c8-5786f2a45331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.863702Z",
     "iopub.status.busy": "2023-01-06T13:07:10.863441Z",
     "iopub.status.idle": "2023-01-06T13:07:10.931544Z",
     "shell.execute_reply": "2023-01-06T13:07:10.930036Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.863660Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def cron():\n",
    "    \"\"\" реуглярно собираем данные из источников и тут же записываем их в SQL\"\"\"\n",
    "    get_all_rss_data()\n",
    "    \n",
    "    load_newest_feeddirs_directly_to_sql()\n",
    "    \n",
    "# тест\n",
    "if \"DEBUG\" in logger.name:\n",
    "    cron()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20a0a6e-2297-4628-8872-dad946709d4b",
   "metadata": {},
   "source": [
    "# Получение объединенной SQL таблицы для всех источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "15259442-4ecc-4d38-8d09-c1b6b319a6ac",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:07:10.933811Z",
     "iopub.status.busy": "2023-01-06T13:07:10.933555Z",
     "iopub.status.idle": "2023-01-06T13:07:11.007913Z",
     "shell.execute_reply": "2023-01-06T13:07:11.006922Z",
     "shell.execute_reply.started": "2023-01-06T13:07:10.933781Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def make_union_main_table(main_table=MAIN_TABLE_NAME):\n",
    "    \"\"\" сливает все имеющиеся таблицы с данными из источников в одну - \n",
    "        имя главной таблицы по умолчанию MAIN_TABLE_NAME\n",
    "    \"\"\"\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config()\n",
    "    \n",
    "    # если пришел пустой список - страшно ругаемся \n",
    "    if len(rss_urls) == 0:\n",
    "        logger.error(f'Стоп! Список источников пуст: {len(rss_urls)}')\n",
    "        raise IOError\n",
    "        \n",
    "    rss_tablenames = [rssname_to_dirname(url) for url in rss_urls]\n",
    "    \n",
    "    \n",
    "    # не удалось использовать SELECT * INTO ... в DBeaver работает, а здесь нет..\n",
    "    # приходится вручную создавать таблицу\n",
    "    qc = f'CREATE TABLE \"{main_table}\" (\\\n",
    "    title text NULL,\\\n",
    "    link text NULL,\\\n",
    "    publish_date timestamptz NULL,\\\n",
    "    category text NULL,\\\n",
    "    description text NULL,\\\n",
    "    \"source\" text NULL,\\\n",
    "    hash text NOT NULL,\\\n",
    "    CONSTRAINT \"main_table_pk\" PRIMARY KEY (hash)\\\n",
    "    );'\n",
    "    \n",
    "    q = f'DROP TABLE IF EXISTS \"{main_table}\"; '\n",
    "    \n",
    "    qq = f'INSERT INTO \"{main_table}\" SELECT * FROM \"{rss_tablenames[0]}\" '\n",
    "     \n",
    "    qqq = ' '.join( [f'UNION SELECT * FROM \"{u}\" ' for u in rss_tablenames[1:]] )\n",
    "    \n",
    "    qqqq = qq + qqq + ' ;'\n",
    "    \n",
    "    # with SQL_ENGINE.connect() as con:\n",
    "    #     res = con.execute(q)\n",
    "    #     res = con.execute(qqqq)\n",
    "    res = SQL_ENGINE.execute(q)\n",
    "    res = SQL_ENGINE.execute(qc)\n",
    "    res = SQL_ENGINE.execute(qqqq)\n",
    "    \n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{main_table}\"')\n",
    "    # общее количество строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "        \n",
    "        \n",
    "    logger.debug(f'Создали объединенную таблицу {main_table}. Количетво записей: {num_str}')\n",
    "    \n",
    "    ##должно получаться как-то вот так\n",
    "    #     SELECT * INTO main \n",
    "    # \t  \t\t   FROM \"habr.com|ru|rss|all|all|\"\n",
    "    # UNION SELECT * FROM \"hibinform.ru|feed|\"\n",
    "    # UNION SELECT * FROM \"lenta.ru|rss|\"\n",
    "    # UNION SELECT * FROM \"regnum.ru|rss\"\n",
    "    # UNION SELECT * FROM \"ria.ru|export|rss2|archive|index.xml\"\n",
    "    # UNION SELECT * FROM \"rossaprimavera.ru|rss\"\n",
    "    # UNION SELECT * FROM \"tass.ru|rss|v2.xml\"\n",
    "    # UNION SELECT * FROM \"www.cnews.ru|inc|rss|news.xml\"\n",
    "    # UNION SELECT * FROM \"www.kommersant.ru|RSS|news.xml\"\n",
    "    # UNION SELECT * FROM \"www.vedomosti.ru|rss|news\"\n",
    "        \n",
    "    # logger.debug(f'Строка запроса: {qqqq} ===')\n",
    "    # return qqqq\n",
    "        \n",
    "\n",
    "# # Тест:\n",
    "# q=''\n",
    "# qq=''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#      make_union_main_table()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e2d96-3913-4c4a-9fd6-c75ed9000324",
   "metadata": {},
   "source": [
    "# Группировка тематических рубрик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d89b32c-dfb6-48fb-b4ce-cada5408392e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-30T20:33:41.007465Z",
     "iopub.status.busy": "2022-12-30T20:33:41.007054Z",
     "iopub.status.idle": "2022-12-30T20:33:41.012880Z",
     "shell.execute_reply": "2022-12-30T20:33:41.011641Z",
     "shell.execute_reply.started": "2022-12-30T20:33:41.007400Z"
    }
   },
   "source": [
    "## загрузка групп категорий из файла в таблицу SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "c29bedce-1fc0-4237-9cb8-bc2b387833ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T13:09:06.678437Z",
     "iopub.status.busy": "2023-01-06T13:09:06.677021Z",
     "iopub.status.idle": "2023-01-06T13:09:07.418806Z",
     "shell.execute_reply": "2023-01-06T13:09:07.417053Z",
     "shell.execute_reply.started": "2023-01-06T13:09:06.678361Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 16:09:06,723:DEBUG.MY_RSS_DATA:DEBUG=>Прочитан файл сводных категорий. Кол-во записей: 216<=2000337152.py->load_category_map_from_file[12]\n",
      "2023-01-06 16:09:07,324:DEBUG.MY_RSS_DATA:DEBUG=>Сводные категории загружены в SQL таблицу category_map. Кол-во записей: 216<=2000337152.py->load_category_map_from_file[30]\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>category</th>\n",
       "      <th>cat_group</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Политика</td>\n",
       "      <td>политика</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Мир</td>\n",
       "      <td>за рубежом</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Общество</td>\n",
       "      <td>общество</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Бизнес</td>\n",
       "      <td>бизнес</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Лента новостей</td>\n",
       "      <td>новости</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>211</th>\n",
       "      <td>Блог компании KTS</td>\n",
       "      <td>бизнес</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>212</th>\n",
       "      <td>Блог компании Белая Радуга</td>\n",
       "      <td>бизнес</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>213</th>\n",
       "      <td>Промышленность</td>\n",
       "      <td>бизнес</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>214</th>\n",
       "      <td>Блог компании AGIMA</td>\n",
       "      <td>бизнес</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>215</th>\n",
       "      <td>Новости агентства</td>\n",
       "      <td>новости</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>216 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                       category   cat_group\n",
       "0                      Политика    политика\n",
       "1                           Мир  за рубежом\n",
       "2                      Общество    общество\n",
       "3                        Бизнес      бизнес\n",
       "4                Лента новостей     новости\n",
       "..                          ...         ...\n",
       "211           Блог компании KTS      бизнес\n",
       "212  Блог компании Белая Радуга      бизнес\n",
       "213              Промышленность      бизнес\n",
       "214         Блог компании AGIMA      бизнес\n",
       "215           Новости агентства     новости\n",
       "\n",
       "[216 rows x 2 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def load_category_map_from_file(cat_file =CATEGORY_FILE, cat_tab=CATEGORY_TABLE):\n",
    "    \"\"\" формирование таблицы сводных категорий из внешнего файла\n",
    "        .. пока такой вариант.\n",
    "    \"\"\"\n",
    "    df = pd.read_csv(cat_file, sep=';', header=None).rename(columns={0:'category', 1:'cat_group'})\n",
    "    \n",
    "    # если пришел пустой список - страшно ругаемся \n",
    "    if len(df) == 0:\n",
    "        logger.error(f'Стоп! Список категорий в файле {cat_file} пуст: {len(rss_urls)}')\n",
    "        raise IOError\n",
    "    \n",
    "    logger.debug(f'Прочитан файл сводных категорий. Кол-во записей: {len(df)}')\n",
    "    \n",
    "    df.to_sql(cat_tab, SQL_ENGINE, if_exists='replace', index=False)\n",
    "    \n",
    "#     # ключ\n",
    "#     q = f'ALTER TABLE public.\"{cat_tab}\" ADD CONSTRAINT \"{cat_tab}_pk\" PRIMARY KEY (category);'\n",
    "#     res = SQL_ENGINE.execute(q)\n",
    "\n",
    "    # добавление суррогатного ключа\n",
    "    q = f'ALTER TABLE {cat_tab} ADD COLUMN id SERIAL PRIMARY KEY;'\n",
    "    res = SQL_ENGINE.execute(q)\n",
    "    \n",
    "    # считаем общее количество получившихся строк\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{cat_tab}\"')\n",
    "    # общее количество строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "    \n",
    "    \n",
    "    logger.debug(f'Сводные категории загружены в SQL таблицу {cat_tab}. Кол-во записей: {num_str}')\n",
    "    \n",
    "    return  df\n",
    "\n",
    "#тест\n",
    "df=''\n",
    "if \"DEBUG\" in logger.name:\n",
    "    df = load_category_map_from_file()\n",
    "df   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed387874-6aea-449e-a433-0f278d371967",
   "metadata": {},
   "source": [
    "## Добавление к главной обобщающей таблице групп категорий"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "6743213c-b5e4-4184-8cc0-7966f323e913",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T14:16:25.671374Z",
     "iopub.status.busy": "2023-01-06T14:16:25.671086Z",
     "iopub.status.idle": "2023-01-06T14:16:27.801746Z",
     "shell.execute_reply": "2023-01-06T14:16:27.800984Z",
     "shell.execute_reply.started": "2023-01-06T14:16:25.671341Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-01-06 17:16:25,697:DEBUG.MY_RSS_DATA:DEBUG=>Удаление главной таблицы: <sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7fed264b1810><=1574772596.py->add_cat_group_to_main_table[10]\n",
      "2023-01-06 17:16:25,810:DEBUG.MY_RSS_DATA:DEBUG=>Создание новой главной таблицы. Кол-во записей: -1<=1574772596.py->add_cat_group_to_main_table[27]\n",
      "2023-01-06 17:16:27,471:DEBUG.MY_RSS_DATA:DEBUG=>Добавеление в главную таблицу категрий. Кол-во записей: 56681<=1574772596.py->add_cat_group_to_main_table[35]\n",
      "2023-01-06 17:16:27,796:DEBUG.MY_RSS_DATA:DEBUG=>Корректировка неучтенных категорий. Кол-во записей: 3489<=1574772596.py->add_cat_group_to_main_table[46]\n",
      "2023-01-06 17:16:27,798:DEBUG.MY_RSS_DATA:DEBUG=>Сформирована главная таблица с категориями. Всего записей: 3489<=1574772596.py->add_cat_group_to_main_table[57]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<sqlalchemy.engine.cursor.LegacyCursorResult object at 0x7fecd7f4f760>\n"
     ]
    }
   ],
   "source": [
    "def add_cat_group_to_main_table():\n",
    "    \"\"\"\n",
    "        Добавление к главной обобщающей таблице групп категорий\n",
    "    \"\"\"\n",
    "     \n",
    "    \n",
    "    res = SQL_ENGINE.execute(f'\\\n",
    "            DROP TABLE IF EXISTS \"{MAIN_TABLE_NAME}_cat\";\\\n",
    "    ')\n",
    "    logger.debug(f'Удаление главной таблицы: {res}')\n",
    "\n",
    "    # формируем новую главную таблицу для вставки туда категрий\n",
    "    res = SQL_ENGINE.execute(f'\\\n",
    "        CREATE TABLE \"{MAIN_TABLE_NAME}_cat\" (\\\n",
    "            title text  NULL,\\\n",
    "            link text  NULL,\\\n",
    "            publish_date timestamptz  NULL,\\\n",
    "            category text NULL,\\\n",
    "            description text  NULL,\\\n",
    "            \"source\" text  NULL,\\\n",
    "            hash text  NULL,\\\n",
    "            cat_group text NULL,\\\n",
    "            cat_id INTEGER NULL,\\\n",
    "            CONSTRAINT \"main_table_cat_pk\" PRIMARY KEY (hash)\\\n",
    "        );\\\n",
    "    ')\n",
    "    logger.debug(f'Создание новой главной таблицы. Кол-во записей: {res.rowcount}')\n",
    "\n",
    "    # вставляем в основную таблицу категрии новостей\n",
    "    res = SQL_ENGINE.execute(f'\\\n",
    "        INSERT INTO \"{MAIN_TABLE_NAME}_cat\" \\\n",
    "        SELECT m.*, cm.cat_group, cm.id as cat_id FROM \"{MAIN_TABLE_NAME}\" m \\\n",
    "        LEFT JOIN \"{CATEGORY_TABLE}\" cm  ON m.category = cm.category;\\\n",
    "    ')\n",
    "    logger.debug(f'Добавеление в главную таблицу категрий. Кол-во записей: {res.rowcount}')\n",
    "    \n",
    "    \n",
    "    # т.к. в текущей версии костыльных ручных групп новостей будут не все группы, то \n",
    "    # пустые группы новостей пронумеруем и поименуем вручную еще раз\n",
    "    res = SQL_ENGINE.execute(f\"\\\n",
    "        UPDATE {MAIN_TABLE_NAME}_cat \\\n",
    "        SET cat_group = 'новые новости' \\\n",
    "            ,cat_id = 0 \\\n",
    "        WHERE cat_group IS NULL OR cat_id IS NULL; \\\n",
    "    \")\n",
    "    logger.debug(f'Корректировка неучтенных категорий. Кол-во записей: {res.rowcount}')\n",
    "\n",
    "    # q = f'INSERT INTO \"{MAIN_TABLE_NAME}_cat\" \\\n",
    "    # (SELECT m.*, cm.cat_group FROM \"{MAIN_TABLE_NAME}\" m \\\n",
    "    # LEFT JOIN \"{CATEGORY_TABLE}\" cm  ON m.category = cm.category) \\\n",
    "    # UNION ALL \\\n",
    "    # (SELECT m.*, cm.category as \"cat_group\" FROM \"{MAIN_TABLE_NAME}\" m, \"{CATEGORY_TABLE}\" cm  \\\n",
    "    # WHERE  m.category != cm.category)\\\n",
    "    # ;'\n",
    "\n",
    "    # print(q)\n",
    "    logger.debug(f'Сформирована главная таблица с категориями. Всего записей: {res.rowcount}')\n",
    "    return res\n",
    "    \n",
    "#тест\n",
    "res = ''\n",
    "if \"DEBUG\" in logger.name:\n",
    "    res = add_cat_group_to_main_table()\n",
    "# print (res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "cd638d0d-a1e9-4d7d-a7c6-6ac8f2f68d2f",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-01-06T14:14:58.886880Z",
     "iopub.status.busy": "2023-01-06T14:14:58.886080Z",
     "iopub.status.idle": "2023-01-06T14:14:58.897528Z",
     "shell.execute_reply": "2023-01-06T14:14:58.895691Z",
     "shell.execute_reply.started": "2023-01-06T14:14:58.886830Z"
    },
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3489"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "res.rowcount"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016f50a-7e94-4e50-a85e-2d8e2e1f4095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-27T13:37:59.864028Z",
     "iopub.status.busy": "2022-12-27T13:37:59.863537Z",
     "iopub.status.idle": "2022-12-27T13:37:59.882195Z",
     "shell.execute_reply": "2022-12-27T13:37:59.881373Z",
     "shell.execute_reply.started": "2022-12-27T13:37:59.863994Z"
    }
   },
   "source": [
    "## --Тематическое моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ffd7634f-9a70-4054-8c91-71ba6457a9dd",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c415308-bb09-4edb-9c78-15d681200794",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,auto:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

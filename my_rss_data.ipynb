{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f853482b-446a-49da-b279-499c800f4faf",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Библиотеки"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "6a99891b-2767-477e-bcc4-bba3142fd564",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:21.946574Z",
     "iopub.status.busy": "2022-12-29T18:01:21.945923Z",
     "iopub.status.idle": "2022-12-29T18:01:25.037456Z",
     "shell.execute_reply": "2022-12-29T18:01:25.036468Z",
     "shell.execute_reply.started": "2022-12-29T18:01:21.946431Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "from rss_parser import Parser\n",
    "from requests import get\n",
    "import pandas as pd\n",
    "import os \n",
    "import json\n",
    "import time\n",
    "from datetime import datetime, timezone\n",
    "\n",
    "import sqlalchemy as sa\n",
    "from sqlalchemy import create_engine, MetaData,Table, Column, Numeric, Integer, VARCHAR, text, DateTime \n",
    "from sqlalchemy.engine import result\n",
    "\n",
    "import errno"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4990422e-fc88-4d41-9072-12c6bc776773",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-26T13:24:01.425526Z",
     "iopub.status.busy": "2022-12-26T13:24:01.364450Z",
     "iopub.status.idle": "2022-12-26T13:24:02.598235Z",
     "shell.execute_reply": "2022-12-26T13:24:02.546571Z",
     "shell.execute_reply.started": "2022-12-26T13:24:01.405898Z"
    },
    "tags": []
   },
   "source": [
    "# Конфигурационные настройки"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1170d9f-adf8-4715-bdf2-921cd7916d36",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Логирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1e2951a8-7413-4892-a3d1-7047f1a911cc",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.039962Z",
     "iopub.status.busy": "2022-12-29T18:01:25.039581Z",
     "iopub.status.idle": "2022-12-29T18:01:25.051649Z",
     "shell.execute_reply": "2022-12-29T18:01:25.050666Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.039929Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# логирование\n",
    "# PRJ_DIR = \"\" #'/home/fedorov/mypy/vk_prj/'\n",
    "# if PRJ_DIR not in sys.path:\n",
    "#     sys.path.insert(0, PRJ_DIR)\n",
    "##########################################\n",
    "# логирование\n",
    "# лучше бы использовать loguru\n",
    "import logging\n",
    "import logging.config\n",
    "\n",
    "# название программы - для логов\n",
    "PROG_NAME = 'MY_RSS_DATA'\n",
    "\n",
    "\n",
    "dictLogConfig = {\n",
    "    \"version\":1,\n",
    "    \"handlers\":{\n",
    "        \"StreamHandler\":{\n",
    "            \"class\":\"logging.StreamHandler\",\n",
    "            \"formatter\":\"myFormatter\"\n",
    "        },\n",
    "        \"GlobalfileHandler\":{\n",
    "            \"class\":\"logging.handlers.RotatingFileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"LOG_{PROG_NAME}.LOG\",\n",
    "            \"backupCount\": 10\n",
    "        },\n",
    "        \"fileHandlerDEBUG\":{\n",
    "            \"class\":\"logging.FileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"DEBUG_{PROG_NAME}.LOG\"\n",
    "        },\n",
    "         \"fileHandlerINFO\":{\n",
    "            \"class\":\"logging.FileHandler\",\n",
    "            \"formatter\":\"myFormatter\",\n",
    "            \"filename\": f\"LOG_{PROG_NAME}.LOG\"\n",
    "        },\n",
    "    },\n",
    "    \"loggers\":{\n",
    "        \"DEBUG\":{\n",
    "            \"handlers\":[\"fileHandlerDEBUG\", \"StreamHandler\"],\n",
    "            \"level\":\"DEBUG\",\n",
    "        },\n",
    "        \"INFO\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\"],\n",
    "            \"level\":\"INFO\",\n",
    "        },\n",
    "        \"WARNING\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"WARNING\",\n",
    "        },\n",
    "        \"ERROR\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"ERROR\",\n",
    "        },\n",
    "        \"CRITICAL\":{\n",
    "            \"handlers\":[\"fileHandlerINFO\", \"GlobalfileHandler\"],\n",
    "            \"level\":\"CRITICAL\",\n",
    "        }\n",
    "    },\n",
    "    \"formatters\":{\n",
    "        \"myFormatter\":{\n",
    "            \"format\":\"%(asctime)s:%(name)s:%(levelname)s=>%(message)s<=%(filename)s->%(funcName)s[%(lineno)d]\"\n",
    "        }\n",
    "    }\n",
    "}\n",
    "logging.config.dictConfig(dictLogConfig)\n",
    "\n",
    "\n",
    "logger = logging.getLogger(\"INFO.\"+PROG_NAME)\n",
    "# logger = logging.getLogger(\"DEBUG.\"+PROG_NAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8aeeeedb-0d22-4e32-94e8-1ca3c15217db",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-27T13:48:44.405275Z",
     "iopub.status.busy": "2022-12-27T13:48:44.404972Z",
     "iopub.status.idle": "2022-12-27T13:48:44.409898Z",
     "shell.execute_reply": "2022-12-27T13:48:44.409051Z",
     "shell.execute_reply.started": "2022-12-27T13:48:44.405243Z"
    },
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Глобальные переменные"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "b733d7ab-e03a-4c4d-b4a1-07d10018bdc5",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.053455Z",
     "iopub.status.busy": "2022-12-29T18:01:25.052965Z",
     "iopub.status.idle": "2022-12-29T18:01:25.284093Z",
     "shell.execute_reply": "2022-12-29T18:01:25.283057Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.053429Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# конфигурационные настройки\n",
    "CONFIG_FILE_NAME = os.path.abspath(u'./config/rss_links.csv')\n",
    "DATA_DIR_NAME = os.path.abspath(u'./data')\n",
    "\n",
    "PGS_LGIN = 'postgres'\n",
    "PGS_PSWD = 'postgres'\n",
    "PGS_DB = 'postgres'\n",
    "PGS_ADDR =  '172.17.0.1' #'192.168.144.9'\n",
    "PGS_PORT = 5440\n",
    "\n",
    "SQL_ENGINE = create_engine(f'postgresql://{PGS_LGIN}:{PGS_PSWD}@localhost:{PGS_PORT}/{PGS_DB}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d17a85-f14e-4277-8cc7-c1a7c4d18bbf",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Чтение конфига с адресами источников РСС"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "847c1b74-ee66-4bf1-abf9-dd9100a0e0ae",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.285949Z",
     "iopub.status.busy": "2022-12-29T18:01:25.285455Z",
     "iopub.status.idle": "2022-12-29T18:01:25.292101Z",
     "shell.execute_reply": "2022-12-29T18:01:25.291337Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.285925Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# читаем конфиг со ссылками на источники\n",
    "def read_config(CONFIG_FILE_NAME):\n",
    "    \"\"\"читаем конфиг со ссылками на источники\n",
    "        CONFIG_FILE_NAME - имя файла с конфигом (если не в локальной директории то с путём)\n",
    "    \"\"\"\n",
    "    df_config = pd.read_csv(CONFIG_FILE_NAME, header=None  )\n",
    "    rss_urls = list(df_config[0])\n",
    "    logger.debug(f'Ссылки на источники прочитаны из {CONFIG_FILE_NAME}')\n",
    "    return rss_urls\n",
    "\n",
    "\n",
    "# Тест\n",
    "# rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "# rss_urls"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "61ef5ad2-62d0-4712-b1a4-a525cfd22ac8",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Подготовка первичного хранилища для данных из источников"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ea4fba1-4c6b-4e8e-bd5d-0ba78639381e",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.294786Z",
     "iopub.status.busy": "2022-12-29T18:01:25.294586Z",
     "iopub.status.idle": "2022-12-29T18:01:25.313909Z",
     "shell.execute_reply": "2022-12-29T18:01:25.312861Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.294762Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def rssname_to_dirname(rss_url:str):\n",
    "    \"\"\" из адреса ссылки на источник делает имя папки для хранения фидов из этого источника\n",
    "        Результат: название папки с фидами источника\n",
    "    \"\"\"\n",
    "    # rss_url = 'https://regnum.ru/rss'# 'https://ria.ru/export/rss2/archive/index.xml' #'https://lenta.ru/rss/' # rss_urls[0]\n",
    "    rss_dirname = rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") \n",
    "    # abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    return rss_dirname\n",
    "\n",
    "    \n",
    "# подготовить: проверить и если надо создать каталог под данные из источника\n",
    "def rss_dir_prepare(rss_url):\n",
    "    \"\"\" Проверить есть ли каталог для данного источника,\n",
    "        Если нет, то создать каталог для сохранения сведений из источника .\n",
    "        rss_url - ссылка на источник из конфиг-файла\n",
    "    \"\"\"\n",
    "    # получаем имя папки с данными из ссылки на источник\n",
    "    rss_dir_name = rssname_to_dirname(rss_url)# rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\")\n",
    "    logger.debug(f'Проверяется папка rss_dir_name = {rss_dir_name}')\n",
    "    \n",
    "    # полный путь до папки с данными\n",
    "    rss_full_dir_name = os.path.join(DATA_DIR_NAME , rss_dir_name ) \n",
    "    rss_abs_dir_name =  rss_full_dir_name #os.path.abspath(rss_full_dir_name)\n",
    "    \n",
    "    # если такой папки еще нет - то создаем\n",
    "    if not os.path.exists(rss_abs_dir_name):\n",
    "        os.mkdir(rss_abs_dir_name)\n",
    "        logger.debug(f'Создна папка {rss_abs_dir_name}')\n",
    "    \n",
    "    return rss_abs_dir_name\n",
    "\n",
    "# Тест:    \n",
    "# rss_url = 'https://lenta.ru/rss/' # rss_urls[0]\n",
    "# rss_dirname = rss_dir_prepare(rss_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b93c1c4-29ea-40b8-bada-1e2556f71bd5",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Получение данных из источника по ссылке "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "5ea64357-7f3d-4e7f-a549-2239004d83b0",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.315565Z",
     "iopub.status.busy": "2022-12-29T18:01:25.315184Z",
     "iopub.status.idle": "2022-12-29T18:01:25.400273Z",
     "shell.execute_reply": "2022-12-29T18:01:25.399327Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.315529Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# получение данных из источника по ссылке rss_url \n",
    "def get_rss(url : str):\n",
    "    \"\"\" получение данных из источника по ссылке rss_url \n",
    "        Результат: словарь feed\n",
    "    \"\"\"\n",
    "    # получаем данны из источника - всю порцию,которую он отдает. Настроек по выбору времени там нет!\n",
    "    xml = get(url)\n",
    "    parser = Parser(xml=xml.content  ) \n",
    "    feed = parser.parse()\n",
    "    logger.debug(f'Данные из {url} получены. Кол-во записей: { len( feed.dict()[\"feed\"]) }. Код Ок: {xml.ok}')\n",
    "    return feed.dict()['feed']\n",
    "\n",
    "# Тест:\n",
    "# rss_url = 'https://lenta.ru/rss/' # rss_urls[0]\n",
    "# rss_feed = get_rss(rss_url)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86bce3d6-4b63-4186-9f06-fff131be8e08",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# Сохранение полученных из истончика данных RSS в файл"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "ccdbcb0c-e979-44ba-adca-229d9a745567",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.401699Z",
     "iopub.status.busy": "2022-12-29T18:01:25.401522Z",
     "iopub.status.idle": "2022-12-29T18:01:25.473083Z",
     "shell.execute_reply": "2022-12-29T18:01:25.471970Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.401679Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "# преобразование даты из строки в datetime с timezone\n",
    "def convert_to_tz_datetime(dt : str): \n",
    "    \"\"\" преобразование даты из строки в datetime с timezone\n",
    "    \"\"\"\n",
    "    # формат даты #'Sat, 24 Dec 2022 09:10:22 +0300'   \n",
    "    fmt = \"%a, %d %b %Y %H:%M:%S %z\" \n",
    "    # код таймзоны\n",
    "    tz = datetime.strptime('+0300', '%z').tzinfo\n",
    "    \n",
    "    rez = datetime.now().astimezone(tz).strptime(dt, fmt)\n",
    "    logger.debug(rez.strftime(fmt) )\n",
    "    return rez\n",
    "\n",
    "\n",
    "# сохранение полученного и распаршенного rss в файл\n",
    "def save_rss_feed(feed_dict : dict, dir_to_save :str):\n",
    "    \"\"\" сохранение полученного и распаршенного rss в файл\n",
    "        вх: rss_feed - словарь с новостями\n",
    "            dir_to_save - путь до директории сохранения\n",
    "    \"\"\"\n",
    "    # формирование имени файла, в который записывается порция данных rss\n",
    "    # текущий таймстамп - для уникального имени файла\n",
    "    now_timestamp = int(datetime.now().timestamp())\n",
    "    \n",
    "    # #даты первой и последней новости в порции рсс\n",
    "    # pub_date_to = convert_to_tz_datetime( rss_feed[0]['publish_date'] )\n",
    "    # pub_date_from = convert_to_tz_datetime( rss_feed[-1]['publish_date'] )\n",
    "\n",
    "    # #имя файла для сохранения порции рсс\n",
    "    # fmt = \"%Y-%m-%d_%H-%M-%S\"\n",
    "    # file_name_dic = {'to':pub_date_to.strftime(fmt), 'from': pub_date_from.strftime(fmt) }\n",
    "    # file_name_str = json.dumps(file_name_dic).replace(\": \",'|')\n",
    "    # file_name_str\n",
    "    \n",
    "    # сохранение полученной порции rss в директорию источника\n",
    "\n",
    "    # полное имя файла для записи\n",
    "    abs_filename = os.path.join(dir_to_save, str(now_timestamp) + '.json')\n",
    "    with open(abs_filename, mode=\"w\") as fp:\n",
    "        json.dump(feed_dict , fp )\n",
    "        logger.debug(f'Rss_feed записан в файл {abs_filename}')\n",
    "    \n",
    "    return abs_filename\n",
    "\n",
    "# Тест:    \n",
    "# rss_filename = save_rss_feed(rss_feed, rss_dirname)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e4e8f2-4aae-4fdf-874f-df4bafc85fa0",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "# ** Загрузка данных из всех источников RSS и запись их в файлы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7108bf18-6176-48d9-a31e-52d04014d50b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.474679Z",
     "iopub.status.busy": "2022-12-29T18:01:25.474262Z",
     "iopub.status.idle": "2022-12-29T18:01:25.550751Z",
     "shell.execute_reply": "2022-12-29T18:01:25.549258Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.474644Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def get_all_rss_data():\n",
    "    \"\"\" Получение данных из всех источников и запись их в файлы\n",
    "        Для CRONa\n",
    "    \"\"\"\n",
    "    logger.info('=== Начало загрузки данных ===')\n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    for url in rss_urls:\n",
    "\n",
    "        # подготавливаем папки для хранения скачиваемых из РСС данных\n",
    "        dirname = rss_dir_prepare(url)\n",
    "\n",
    "        # получаем порцию данных по ссылке\n",
    "        feed = get_rss(url)\n",
    "\n",
    "        # сохраняем данные в заранее подготовленной папке\n",
    "        rez_filename = save_rss_feed(feed, dirname)\n",
    "        \n",
    "    logger.info(f'=== Данные загрузили. Кол-во источников {len(rss_urls)} ===')\n",
    "        \n",
    "\n",
    "# # Тест:\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     get_all_rss_data()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e973f857-9cce-4f95-8988-55368b0977ba",
   "metadata": {
    "tags": []
   },
   "source": [
    "# Инициализирующая Загрузка данных из файлов в хранилище (SQL БД)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16d10eca-d4d8-4e3c-82e5-ed63dd0170a6",
   "metadata": {
    "incorrectly_encoded_metadata": "jp-MarkdownHeadingCollapsed=true",
    "tags": []
   },
   "source": [
    "## Прочитать файл feed и сделать из него таблицу пандас"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "e63fbbd8-9aba-4ef0-a30b-ce73cfd158d8",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.555361Z",
     "iopub.status.busy": "2022-12-29T18:01:25.555067Z",
     "iopub.status.idle": "2022-12-29T18:01:25.608549Z",
     "shell.execute_reply": "2022-12-29T18:01:25.607670Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.555328Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "# прочитать из фид-файла и записать в пандас датафрейм\n",
    "def feedfile_to_pandas(rss_url:str, rss_file_name:str):\n",
    "    \"\"\" Читает json файл с сохраненнымto_list преобразует его в таблицу пандас\n",
    "        rss_url - название папки с файлами-фидами источника\n",
    "        rss_file_name - имя файла с фидом\n",
    "        Результат: таблица пандас\n",
    "    \"\"\"\n",
    "    \n",
    "    # формируем полное имя файла\n",
    "    rss_dirname = rssname_to_dirname(rss_url) \n",
    "    rss_full_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    feed_filename = os.path.join(rss_full_dirname, rss_file_name)\n",
    "    \n",
    "    \n",
    "    # открываем первый файл - это самый новый, т.к. сотритовка обратная\n",
    "    feed=''\n",
    "    with open(feed_filename, 'r') as fp:\n",
    "        feed = json.load(fp)\n",
    "        logger.debug(f'Прочитали содержимое файла {feed_filename}. Кол-во записей: {len(feed)}')\n",
    "\n",
    "    # закидываем фид в пандас : колонки только те, которые нужны\n",
    "    columns = ['title', 'link', 'publish_date', 'category', 'description' ] # 'description_links', 'description_images', 'enclosure', 'itunes'\n",
    "    df = pd.json_normalize(feed)[columns]\n",
    "    # добавляем признак источника\n",
    "    df['source'] = rss_dirname\n",
    "    df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "    df['hash'] = pd.util.hash_pandas_object(df[['title','category', 'source']]).astype('str')\n",
    "    \n",
    "    logger.debug(f'Из файла {feed_filename} получили таблицу, кол-во строк {len(df)}.')\n",
    "    return df\n",
    "\n",
    "# тест feedfile_to_pandas\n",
    "# rss_url = 'https://regnum.ru/rss'\n",
    "# feed_filename = '1672120674.json'\n",
    "# df1 = feedfile_to_pandas(rss_url, feed_filename)\n",
    "# df1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fe173428-f5a0-4842-8f12-364b13e22a65",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 1. ** Начальная инициализация через Pandas: Объединить в Pandas все файлы из папки источника рсс и записать результат в хранилище"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "5be55134-7d44-4c12-a18c-55b4bd650846",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.610419Z",
     "iopub.status.busy": "2022-12-29T18:01:25.610016Z",
     "iopub.status.idle": "2022-12-29T18:01:25.680944Z",
     "shell.execute_reply": "2022-12-29T18:01:25.679807Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.610387Z"
    },
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def join_all_feedfiles_pandas_sql(rss_url: str):\n",
    "    \"\"\" взять все файлы с фидами в папке рсс, объединить их, убрав повторения и приготовить к записи в хранилище (?БД)\n",
    "        Результат: таблица пандас с уникальными записями из всех файлов в папке источника\n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # получаем список сохраненных файлов\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Кол-во файлов: {len(list_dir)}. Список: {list_dir}')\n",
    "    \n",
    "    df_rez = pd.DataFrame()\n",
    "    \n",
    "    for rf in list_dir:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "        #дату из строки делаем датой\n",
    "        # df['publish_date'] = pd.to_datetime(df['publish_date'])\n",
    "        \n",
    "        #для отладки инфо: превая и последняя запись датафрефма\n",
    "        str_fst = df.iloc[0,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        str_lst = df.iloc[-1,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        logger.debug(f'Таблица для файла:{rf}, строк:{len(df)}, нач.:{str_fst}, кон.:{str_lst}')\n",
    "        # объединяем полученное с имеющимся \n",
    "        if df_rez.empty:\n",
    "            df_rez = df\n",
    "            logger.debug(f'Начальная инициализация пустой таблицы')\n",
    "        df_rez = pd.concat([df_rez, df], ignore_index=True ).df_rez.drop_duplicates(ignore_index=True)\n",
    "    \n",
    "    logger.debug(f'Сформировали сводную таблицу для файлов в {abs_rss_dirname}. Кол-во строк: {len(df_rez)}')\n",
    "    # df_rez.drop_duplicates(ignore_index=True, inplace=True)\n",
    "    # logger.debug(f'После удаления дубликатов: кол-во строк: {len(df_rez)}')\n",
    "    \n",
    "    # добавляем результат в БД\n",
    "    df_rez.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace' )\n",
    "    logger.debug(f'Добавлено в БД в таблицу: {rss_dirname}')\n",
    "    \n",
    "    return df_rez\n",
    "\n",
    "# # тест\n",
    "# if \"DEBUG\" in logger.name:\n",
    "    # rss_url = 'https://regnum.ru/rss'#\n",
    "    # df_rez = join_all_feedfiles_pandas_sql(rss_url)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e73c216-513f-417b-8426-b97f28a71438",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 2. ** Начальная инициализация через SQL: Каждый файлы из папки источника рсс добавить в SQL хранилище, убрав дубликаты"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "36de0afa-6564-4dcf-9fb7-2fd0ec183179",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.682790Z",
     "iopub.status.busy": "2022-12-29T18:01:25.682552Z",
     "iopub.status.idle": "2022-12-29T18:01:25.775276Z",
     "shell.execute_reply": "2022-12-29T18:01:25.774085Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.682759Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_all_feedfiles_sql(rss_url: str):\n",
    "    \"\"\" брать по очереди файлы с фидами в папке рсс и вставлять в SQL таблицу, избегая повторений\n",
    "        Результат: SQL таблица с уникальными записями из всех файлов в папке источника\n",
    "        \n",
    "        от insert_newest_feedfiles_by_sql отличается только тем, что берутся все файлы , а там только превый.\n",
    "        Можно сделать одну функцию и через параметр управлять количеством файлов. \n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # поскольку это начальная инициализация, то имеющуюся SQL таблицу удаляем\n",
    "    q = f'DROP TABLE IF EXISTS \"{rss_dirname}\"'\n",
    "    with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "    \n",
    "    # получаем список сохраненных файлов\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Кол-во файлов: {len(list_dir)}. Список: {list_dir}')\n",
    "    \n",
    "    for rf in list_dir:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "                \n",
    "        # #для отладки инфо: превая и последняя запись датафрефма\n",
    "        # str_fst = df.iloc[0,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        # str_lst = df.iloc[-1,:][['publish_date', 'title']].to_string().replace('  ',\"\").replace('publish_date',\"\").replace('\\ntitle',\"\")[:50]\n",
    "        # logger.debug(f'Таблица для файла:{rf}, строк:{len(df)}, нач.:{str_fst}, кон.:{str_lst}')\n",
    "        # объединяем полученное с имеющимся \n",
    "        # if df_rez.empty:\n",
    "        #     df_rez = df\n",
    "        #     logger.debug(f'Начальная инициализация пустой таблицы')\n",
    "        # df_rez = pd.concat([df_rez, df], ignore_index=True ).df_rez.drop_duplicates(ignore_index=True)\n",
    "        \n",
    "        # если sql таблица с данными для этого источника еще не создана - создаем новую, иначе - дописываем\n",
    "        if rss_dirname not in sa.inspect(SQL_ENGINE).get_table_names():\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ОСНОВНУЮ таблицу для данного источника\n",
    "            df.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            # делаем первичным ключем - хэш, чтобы записи не повторялись\n",
    "            q = f'ALTER TABLE public.\"{rss_dirname}\" ADD CONSTRAINT \"{rss_dirname}_pk\" PRIMARY KEY (hash);'\n",
    "            \n",
    "        else:\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ВРЕМЕННУЮ таблицу\n",
    "            tmp_dbname = \"tmp.\"+rss_dirname\n",
    "            df.to_sql(tmp_dbname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            q = f'INSERT INTO \"{rss_dirname}\" SELECT * FROM \"{tmp_dbname}\" WHERE hash NOT IN (SELECT hash FROM \"{rss_dirname}\")'\n",
    "            \n",
    "        # выполняем сформированный SQl запрос\n",
    "        with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "            \n",
    "\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{rss_dirname}\"')\n",
    "    \n",
    "    # общее количество строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "            \n",
    "    logger.debug(f'Сформировали SQL таблицу \"{rss_dirname}\". Кол-во строк: {num_str}')\n",
    "    \n",
    "    return {rss_dirname:num_str}\n",
    "\n",
    "# # тест\n",
    "# rez_sql = ''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     rss_url = 'https://regnum.ru/rss'#\n",
    "#     rez_sql = insert_all_feedfiles_sql(rss_url)\n",
    "# rez_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "474654bb-34a3-4878-8152-a32285c149f6",
   "metadata": {
    "tags": []
   },
   "source": [
    "## 3. ** Инкрементальная загрузка свежей порции данных через SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "da66b761-6640-4996-a48e-d088e6382a9a",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.779446Z",
     "iopub.status.busy": "2022-12-29T18:01:25.779238Z",
     "iopub.status.idle": "2022-12-29T18:01:25.912400Z",
     "shell.execute_reply": "2022-12-29T18:01:25.910931Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.779424Z"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def insert_newest_feedfiles_by_sql(rss_url: str):\n",
    "    \"\"\" взять самый свежий файл с фидом в папке рсс и встить в SQL таблицу, избегая повторений\n",
    "        Результат: SQL таблица с уникальными записями из всех файлов в папке источника\n",
    "    \"\"\"\n",
    "    # подготавливаем имя папки для чтения скачанных из РСС данных - отдельных файлов\n",
    "    rss_dirname = rssname_to_dirname(rss_url) #rss_url.replace(u'https://', \"\").replace(u\"/\",\"|\") # rss_dir_prepare(rss_url)\n",
    "    abs_rss_dirname = os.path.join(DATA_DIR_NAME, rss_dirname)\n",
    "    \n",
    "    # надо бы проверить - существует такакая таблица или еще нет\n",
    "    \n",
    "    # получаем список сохраненных файлов - сортируем в порядке убывания времени - т.е. самый свежий файл будет первым\n",
    "    list_dir = [ fn for fn in sorted( os.listdir(abs_rss_dirname), reverse=True) if '.json' in fn]\n",
    "    # если файлов не нашлось - страшно ругаемся\n",
    "    if len(list_dir) == 0:\n",
    "        logger.error(\"Стоп! Файлы для добавления в sql таблицу {rss_dirname} отсутствуют в папке {abs_rss_dirname}\")\n",
    "        raise IOError \n",
    "    logger.debug(f'Прочитали директорию {abs_rss_dirname}. Берем в работу самый свежий файл: {list_dir[0]}')\n",
    "    \n",
    "    for rf in list_dir[0:1]:\n",
    "        # получаем датафрейм пандас для файла\n",
    "        df = feedfile_to_pandas(rss_url, rf)\n",
    "        \n",
    "        # если sql таблица с данными для этого источника еще не создана - создаем новую, иначе - дописываем\n",
    "        if rss_dirname not in sa.inspect(SQL_ENGINE).get_table_names():\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ОСНОВНУЮ таблицу для данного источника\n",
    "            df.to_sql(rss_dirname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            # делаем первичным ключем - хэш, чтобы записи не повторялись\n",
    "            q = f'ALTER TABLE public.\"{rss_dirname}\" ADD CONSTRAINT \"{rss_dirname}_pk\" PRIMARY KEY (hash);'\n",
    "            \n",
    "        else:\n",
    "            # вставляем данные из пандаса прямо в новую создаваемуб ВРЕМЕННУЮ таблицу\n",
    "            tmp_dbname = \"tmp.\"+rss_dirname\n",
    "            df.to_sql(tmp_dbname, SQL_ENGINE, if_exists='replace', index=False)\n",
    "            q = f'INSERT INTO \"{rss_dirname}\" SELECT * FROM \"{tmp_dbname}\" WHERE hash NOT IN (SELECT hash FROM \"{rss_dirname}\")'\n",
    "            \n",
    "        # выполняем сформированный SQl запрос\n",
    "        with SQL_ENGINE.connect() as con:\n",
    "            res = con.execute(q)\n",
    "            \n",
    "\n",
    "    res = SQL_ENGINE.execute(f'SELECT count(*) FROM \"{rss_dirname}\"')\n",
    "    \n",
    "    # финальное количство строк в таблице\n",
    "    num_str = res.first()[0]\n",
    "    \n",
    "    logger.debug(f'Записали в SQL таблицу \"{rss_dirname}\". Кол-во строк: {num_str}')\n",
    "    \n",
    "    return {rss_dirname:num_str}\n",
    "\n",
    "# # тест\n",
    "# rez_sql = ''\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     rss_url = 'https://regnum.ru/rss'#\n",
    "#     rez_sql = insert_newest_feedfiles_by_sql(rss_url)\n",
    "# rez_sql\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "172fcdbc-3d13-4d1a-950f-029db7457508",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## -- Инициализирующая Загрузка данных из всех файлов всех папок источников RSS в SQL через PANDAS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "67814561-a912-4b90-ac56-f20d1567e499",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:25.914667Z",
     "iopub.status.busy": "2022-12-29T18:01:25.913554Z",
     "iopub.status.idle": "2022-12-29T18:01:26.023398Z",
     "shell.execute_reply": "2022-12-29T18:01:26.021911Z",
     "shell.execute_reply.started": "2022-12-29T18:01:25.914588Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_feeddirs_to_sql_by_pandas():\n",
    "    \"\"\" Загрузка всех данных из папок источников в SQL , через объединение их в pandas\"\"\"\n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    for url in rss_urls:\n",
    "\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        join_all_feedfiles_pandas_sql(url)\n",
    "\n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_all_feeddirs_to_sql_by_pandas()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fe28ead-ab43-40cf-b5fc-a321795e493d",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ** Инициализирующая Загрузка данных из ВСЕХ файлов всех папок источников RSS СРАЗУ в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "122303b5-4fa6-4db5-a5a8-652410d2296b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:26.032678Z",
     "iopub.status.busy": "2022-12-29T18:01:26.032073Z",
     "iopub.status.idle": "2022-12-29T18:01:26.074315Z",
     "shell.execute_reply": "2022-12-29T18:01:26.072411Z",
     "shell.execute_reply.started": "2022-12-29T18:01:26.032605Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_all_feeddirs_directly_to_sql():\n",
    "    \"\"\" Инициализирующая Загрузка всех данных из папок источников непосресдвенно в SQL \n",
    "        Если таблица уже была, то она удаляется\n",
    "    \"\"\"\n",
    "    \n",
    "    logger.info(f'== Начало Инициализирующей загрузки')\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    str_num = dict()\n",
    "    \n",
    "    for url in rss_urls:\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        rez = insert_all_feedfiles_sql(url) \n",
    "        str_num.update(rez)\n",
    "        \n",
    "    logger.info(f'== Инициализирующая загрузка произведена. Кол-ва загруженных строк: {str_num}')\n",
    "        \n",
    "\n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_all_feeddirs_directly_to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "11941ef5-f143-4af4-9ea0-2ec4b2375a74",
   "metadata": {
    "tags": []
   },
   "source": [
    "## ** Инкрементальная Загрузка данных для всех источников RSS СРАЗУ в SQL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "1e508fb7-a714-402c-b855-6ce0bbdde3d6",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:26.077730Z",
     "iopub.status.busy": "2022-12-29T18:01:26.076815Z",
     "iopub.status.idle": "2022-12-29T18:01:26.088599Z",
     "shell.execute_reply": "2022-12-29T18:01:26.086971Z",
     "shell.execute_reply.started": "2022-12-29T18:01:26.077660Z"
    },
    "lines_to_next_cell": 2,
    "tags": []
   },
   "outputs": [],
   "source": [
    "def load_newest_feeddirs_directly_to_sql():\n",
    "    \"\"\" Загрузка самых новых данных (самый ноывй файл) из папок источников непосредственно в SQL  \"\"\"\n",
    "    \n",
    "    logger.info('=== Запись свежих данных в SQL таблицы ===')\n",
    "    \n",
    "    # читаем конфиг с адресами источников РСС\n",
    "    rss_urls = read_config(CONFIG_FILE_NAME)\n",
    "\n",
    "    str_num = dict()\n",
    "    \n",
    "    for url in rss_urls:\n",
    "        # группируем все в один датафрейм и записываем его в SQL\n",
    "        rez = insert_newest_feedfiles_by_sql(url)\n",
    "        str_num.update(rez)\n",
    "        \n",
    "    logger.info(f'=== Конец записи. Кол-ва записей в таблицах: {str_num} ===')\n",
    "    \n",
    "\n",
    "# if \"DEBUG\" in logger.name:\n",
    "#     # можно сначала загрузить свежую порцию фидов \n",
    "#     # get_all_rss_data()\n",
    "#     # а потом закинуть все в БД\n",
    "#     load_newest_feeddirs_directly_to_sql()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f15ded28-5144-40c6-acd5-01533fd8cc94",
   "metadata": {
    "tags": []
   },
   "source": [
    "# ** CRON : регулярное получение данных и записывание их в SQL базу"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "eeb99207-30e2-4f97-a2c8-5786f2a45331",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-29T18:01:26.091170Z",
     "iopub.status.busy": "2022-12-29T18:01:26.090694Z",
     "iopub.status.idle": "2022-12-29T18:01:26.148375Z",
     "shell.execute_reply": "2022-12-29T18:01:26.146694Z",
     "shell.execute_reply.started": "2022-12-29T18:01:26.091108Z"
    }
   },
   "outputs": [],
   "source": [
    "def cron():\n",
    "    \"\"\" реуглярно собираем данные из источников и тут же записываем их в SQL\"\"\"\n",
    "    get_all_rss_data()\n",
    "    \n",
    "    load_newest_feeddirs_directly_to_sql()\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b1e2d96-3913-4c4a-9fd6-c75ed9000324",
   "metadata": {},
   "source": [
    "# Группировка тематических рубрик"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a016f50a-7e94-4e50-a85e-2d8e2e1f4095",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2022-12-27T13:37:59.864028Z",
     "iopub.status.busy": "2022-12-27T13:37:59.863537Z",
     "iopub.status.idle": "2022-12-27T13:37:59.882195Z",
     "shell.execute_reply": "2022-12-27T13:37:59.881373Z",
     "shell.execute_reply.started": "2022-12-27T13:37:59.863994Z"
    }
   },
   "source": [
    "## Тематическое моделирование"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c415308-bb09-4edb-9c78-15d681200794",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "jupytext": {
   "formats": "ipynb,py:percent"
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
